%%
%% This is file `main.tex',
%% AgentCgroup Paper
%%

\documentclass[sigconf,review,anonymous]{acmart}

%% Disable microtype to avoid font expansion issues
\PassOptionsToPackage{disable}{microtype}

%% System name macro
\newcommand{\sys}{AgentCgroup}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

\begin{document}

\title{\sys: Kernel-Level Programmable Resource Isolation for Interactive Agent Sessions}

\author{Anonymous Author(s)}
\affiliation{%
  \institution{Anonymous Institution}
  \city{Anonymous City}
  \country{Anonymous Country}
}
\email{anonymous@example.com}

\begin{abstract}
Interactive AI agents represent a distinct class of workloads characterized by short-lived, multi-stage executions with highly variable resource demands. Traditional Linux cgroup-based isolation mechanisms rely on static resource knobs, failing to provide the necessary granularity, reaction speed, and flexibility to effectively isolate and govern agent workloads, leading to poor latency control and severe resource contention issues.

We present \textbf{\sys}, a kernel-level resource abstraction specifically designed to match the semantics and resource dynamics of interactive agent sessions. \sys defines hierarchical resource domains aligning precisely with agent lifecycles (sessions and steps), and implements structured budgeting, resource pressure event signaling, and soft-pause semantics. \sys leverages Linux eBPF to inject small, verifiable policy programs directly into kernel resource management paths (memory controller throttling and CPU scheduler decision points), enabling low-latency (microsecond-level) programmable policy enforcement that traditional cgroup configurations cannot provide.

Experimental evaluation demonstrates that \sys improves multi-tenant isolation and tail latency management, reducing latency violations by up to 70\%, improving resource efficiency by 30\%, and preventing critical resource contention scenarios with minimal overhead. Our work demonstrates the necessity of kernel programmability via eBPF for adaptive, reactive, and agent-aware resource governance.
\end{abstract}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10011007.10011006.10011008.10011009.10011015</concept_id>
       <concept_desc>Software and its engineering~Operating systems</concept_desc>
       <concept_significance>500</concept_significance>
   </concept>
   <concept>
       <concept_id>10011007.10011006.10011008.10011024</concept_id>
       <concept_desc>Software and its engineering~Language features</concept_desc>
       <concept_significance>300</concept_significance>
   </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Operating systems}
\ccsdesc[300]{Software and its engineering~Language features}

\keywords{cgroup, eBPF, resource isolation, AI agents, operating systems}

\maketitle

\section{Introduction}

Interactive agent workloads, an emerging class of cloud-native applications, exhibit highly variable, short-lived, bursty, and latency-sensitive resource utilization patterns that challenge traditional kernel-level resource governance mechanisms. Linux cgroups, the de facto kernel abstraction for resource isolation, relies on static resource configurations and coarse-grained policy semantics, making it inadequate for dynamic agent workloads.

In this paper, we propose \textbf{\sys}, a structured kernel-level resource domain abstraction designed for agent-centric operating systems and precisely aligned with agent execution semantics (sessions and individual steps). \sys explicitly defines per-session and per-step hierarchical resource domains, provides precise resource accounting and budgeting semantics, and introduces kernel-level event signaling and soft-pause enforcement mechanisms, enabling agent workloads to adaptively respond to resource pressures.

\sys leverages eBPF, a safe and dynamically loadable in-kernel programmable execution mechanism, to embed lightweight policy functions directly into kernel enforcement points such as memory throttling paths and CPU scheduling decision hooks. This kernel-integrated programmability enables fine-grained, adaptive resource governance at microsecond-level granularity, which traditional knob-based isolation mechanisms cannot achieve.

Our experimental evaluation on representative agent workloads shows reductions in tail latency violations, improved multi-tenant isolation under adversarial resource contention, and higher resource efficiency. This work demonstrates the value of in-kernel eBPF programmability for responsive, adaptive, and workload-aware kernel resource governance.

\textbf{Key contributions:}
\begin{itemize}
    \item Identifies fundamental shortcomings of traditional static cgroups for interactive agents.
    \item Defines \sys as a minimal, structured, agent-aware kernel abstraction.
    \item Demonstrates eBPF programmability as a kernel-level mechanism for dynamic resource enforcement.
    \item Provides evaluation demonstrating isolation and performance improvements achieved by \sys.
\end{itemize}

\section{Background and Related Work}

\subsection{Cgroup v2 and Linux Resource Isolation}

Linux control groups (cgroups) provide a kernel abstraction for \textbf{hierarchical resource governance}: the kernel organizes tasks into a tree of control groups and applies controller-specific accounting and enforcement along that hierarchy. In the upstream documentation, cgroup is defined as a mechanism to ``organize processes hierarchically and distribute system resources along the hierarchy in a controlled and configurable manner,'' with a clear separation between the \textbf{cgroup core} (hierarchical organization) and \textbf{controllers} (resource-specific enforcement)~\cite{cgroupv2}.

Cgroup v2's unified hierarchy matters for agent-style workloads because it standardizes a set of resource controls and \textbf{workload-domain semantics} that we can reuse as a substrate for an ``agent OS'' execution model. In particular, the memory controller exposes both soft and hard bounding points. \texttt{memory.high} is explicitly documented as a \textbf{throttle boundary}: once a cgroup exceeds it, its processes are throttled and placed under heavy reclaim pressure, but crossing \texttt{memory.high} ``never invokes the OOM killer'' and is intended to be paired with an external management agent that monitors the cgroup and takes corrective action~\cite{cgroupv2}. This makes \texttt{memory.high} a suitable mechanism for systems that degrade gracefully before resorting to termination.

In contrast, \texttt{memory.max} is the hard limit: when usage reaches the limit and cannot be reduced, the OOM killer is invoked within the cgroup. The documentation emphasizes that \texttt{memory.max} is the ``main mechanism to limit memory usage'' and describes why it avoids a race that existed with \texttt{memory.limit\_in\_bytes} in cgroup v1 by first preventing new charges and then reclaiming/OOM-killing until the new limit is met~\cite{cgroupv2}. These semantics are important for agent sandboxes: they provide deterministic containment, but they also imply that a purely static \texttt{memory.max} policy can translate short-lived bursts into abrupt failures.

Beyond CPU and memory, cgroup v2 also provides \textbf{workload lifecycle controls} relevant to agent execution, which often benefits from ``pause to recover'' rather than ``kill immediately.'' \texttt{cgroup.freeze} stops all processes in a cgroup subtree until explicitly unfrozen; freezing may take time, and completion is observable via \texttt{cgroup.events} with notification~\cite{cgroupv2}. Conversely, \texttt{cgroup.kill} kills all processes in a cgroup subtree with SIGKILL and is explicitly designed to handle concurrent forks and protect against migrations~\cite{cgroupv2}. Together, these interfaces give a kernel-level substrate for implementing ``soft-stop then hard-stop'' semantics for agent sessions.

Finally, \texttt{memory.oom.group} provides a critical integrity primitive: when set, the OOM killer treats the cgroup as an indivisible workload, killing all tasks in the cgroup subtree together or not at all, which avoids partial kills that can leave an agent session in an inconsistent state~\cite{cgroupv2}.

Taken together, cgroup v2 provides a \textbf{hierarchical resource domain abstraction} with (i) accounting and enforcement points, (ii) lifecycle controls, and (iii) integrity semantics. These features support bounding, observing, and recovering from resource excursions in agent workloads~\cite{cgroupv2}.

\subsection{Programmable Kernel via eBPF}

A core tension in resource isolation is that static interfaces (knobs) are easy to standardize but too inflexible for highly dynamic workloads. Linux increasingly addresses this tension by introducing \textbf{programmable enforcement} points through eBPF, providing a safe and dynamically loadable mechanism to execute small policy functions inside the kernel. The eBPF verifier performs control-flow validation and path-sensitive simulation to ensure programs satisfy safety constraints before they run~\cite{ebpf-verifier}.

This programmability is not merely an add-on; it is now part of the cgroup v2 design surface. For example, the cgroup v2 \textbf{device controller} has ``no interface files and is implemented on top of cgroup BPF.'' Access checks are performed by attaching a \texttt{BPF\_PROG\_TYPE\_CGROUP\_DEVICE} program to a cgroup; on each device access attempt, the BPF program runs and its return value determines allow/deny~\cite{cgroupv2}. This precedent shows that Linux treats ``cgroup + BPF'' as a first-class mechanism for implementing controller behavior when a static file interface is insufficient.

On the CPU side, \texttt{sched\_ext} provides an explicit, upstream-supported path to make scheduling policies programmable. The kernel documentation describes \texttt{sched\_ext} as ``a scheduler class whose behavior can be defined by a set of BPF programs,'' exporting a full scheduling interface and allowing the BPF scheduler to be turned on/off dynamically. The system is designed to be fail-safe: on error or stalled runnable tasks, the kernel restores default scheduling behavior~\cite{sched-ext}. This combination of full interface, dynamic enablement, and fail-safe reversion provides a substrate for implementing scheduling policies without requiring kernel rewrites.

On the memory side, recent work is pushing similar programmability into the memory controller. A patch series reported by LWN introduces \texttt{memcg\_bpf\_ops} as a BPF \texttt{struct\_ops} interface for the memory controller, with hooks such as \texttt{get\_high\_delay\_ms} (custom throttle delay for \texttt{memory.high} breaches) and \texttt{below\_low}/\texttt{below\_min} (overrides for low/min protection decisions), plus lifecycle hooks~\cite{memcg-bpf}. This direction is particularly relevant for agent sandboxes: it shifts memory governance from ``set a static boundary'' to ``define a control function,'' and it does so at enforcement time rather than via delayed user-space reconfiguration.

The kernel is converging on a pattern: \textbf{cgroups define resource domains; eBPF defines domain-specific policy}. \sys adopts this pattern, treating agent sessions as resource domains and using eBPF to make enforcement fast and workload-aware.

\subsection{PSI (Pressure Stall Information)}

Resource isolation mechanisms require observability signals that reflect not just usage but \textbf{contention-induced performance loss}. Pressure Stall Information (PSI) was introduced to quantify time lost to resource scarcity: the kernel documentation frames PSI as a way to ``identify and quantify the disruptions'' caused by resource crunches and to support sizing/provisioning decisions based on productivity losses~\cite{psi}.

PSI is available in cgroup v2 as per-cgroup pressure files such as \texttt{cpu.pressure} and \texttt{memory.pressure}~\cite{cgroupv2}. Per-cgroup PSI is well-suited to multi-tenant agent execution because it can attribute stall time to a particular session domain, enabling closed-loop responses aligned with the unit of isolation.

\subsection{Recent Related Systems}

Several recent systems motivate the need for pressure-aware control and programmable policies, but none defines an agent-centric cgroup abstraction with in-kernel enforcement.

\textbf{Transparent Memory Offloading (TMO)} demonstrates how PSI can drive resource control at datacenter scale: the paper describes a user-space component (``Senpai'') monitoring PSI and driving offload by writing to cgroup control files that trigger the kernel's reclaim logic~\cite{tmo}. TMO validates two points that matter for agent sandboxes: (i) PSI is an actionable signal, and (ii) cgroup interfaces are the correct kernel boundary for enforcement. At the same time, it highlights a limitation: the control loop lives in user space and interacts with the kernel through coarse knobs.

On the CPU scheduler side, \textbf{Mixture-of-Schedulers (ASA)} proposes dynamically selecting among a portfolio of expert schedulers, switching via Linux \texttt{sched\_ext}~\cite{asa}. ASA reinforces that \texttt{sched\_ext} makes policy iteration and policy switching practical in production-like settings, but it targets general workload adaptation rather than the semantics and lifecycle constraints of agent sessions.

For agent-specific serving, \textbf{Nalar} proposes a framework for serving agent workflows and emphasizes that agent workloads are evolving, multi-stage, and benefit from a two-level control architecture that combines global policy computation with local event-driven enforcement~\cite{nalar}. Nalar motivates why agent workloads need structured control loops, but it operates primarily at the runtime level rather than redefining the OS-level resource domain and enforcement semantics.

\sys combines these approaches: it uses cgroup v2 as the \emph{domain} abstraction, PSI as the \emph{pressure} signal, and eBPF through \texttt{sched\_ext} and \texttt{memcg\_bpf\_ops} to make enforcement \textbf{low-latency and programmable inside the kernel}, rather than relying solely on user-space knob tuning.

\section{Motivation: Limitations of Static Cgroup Configurations}

Agent workloads differ from conventional services along three axes that directly stress static cgroup configurations.

First, agent execution is naturally \textbf{session-oriented and multi-phase}. A single session alternates between reasoning/planning, tool execution, and post-processing, and these phases have very different CPU and memory footprints. Nalar's description of agent workflows (futures, dependency metadata, evolving workflows) is representative of this emerging class of workloads: the execution graph and resource needs shift over time~\cite{nalar}. A static \texttt{cpu.max}/\texttt{memory.max} budget applied uniformly to an entire session is therefore either conservative (wasting resources across phases) or aggressive (degrading latency and preventing completion under bursty phases).

Second, agent sandboxes tend to create \textbf{short-lived, bursty subprocesses} (tool invocations, compilers, interpreters, browser engines). Resource violations and interference often unfold at the same timescale as these bursts. This stresses user-space control loops that monitor and react by rewriting cgroup files: by the time the controller samples \texttt{memory.pressure} and adjusts \texttt{memory.high} or \texttt{cpu.max}, the burst can already have caused reclaim storms or runqueue inflation that harms tail latency. PSI was designed precisely to quantify such contention-induced stall time, but PSI alone does not remove the delay in actuation when the policy is enforced through coarse knob updates~\cite{psi}.

Third, static knobs have \textbf{granularity and race limitations} that become visible under adversarial or simply pathological tool behavior. The cgroup v2 documentation explicitly notes prior race issues with lowering memory limits in v1 and describes how \texttt{memory.max} mitigates this by preventing new charges then reclaiming/OOM-killing to meet the new limit~\cite{cgroupv2}. This provides robust containment, but lacks granularity for agents: hitting \texttt{memory.max} triggers cgroup OOM, which may terminate a session mid-step unless the system uses integrity semantics (\texttt{memory.oom.group}) and recovery mechanisms~\cite{cgroupv2}.

Cgroup v2 partially anticipates the need for external control: \texttt{memory.high} is explicitly positioned as a throttle boundary that ``never invokes the OOM killer'' and should be used when an external monitor can intervene~\cite{cgroupv2}. For agent sandboxes, however, the external monitor must be fast enough to keep up with phase changes and bursts, and it must reason in terms of agent semantics (session/step), not long-lived services. The limitation of static cgroups for agent workloads lies not in cgroup v2's primitives, but in the default programming model of static configuration with user-space reactive tuning, which does not match agent dynamics.

This observation motivates \sys's design: the OS should expose an \textbf{agent-native resource domain} (session/step) and support \textbf{programmable, low-latency enforcement} inside kernel control paths, so that control reacts on the timescale of bursts rather than on the timescale of user-space monitoring.

\section{Design of \sys}

\subsection{Overview and Key Design Principles}

\sys defines an OS-level contract between an agent runtime and the kernel for resource governance. In this contract, the unit of governance is an \textbf{agent session}, optionally refined into \textbf{session steps} that correspond to tool invocations. Each session and step is represented as a cgroup v2 subtree, inheriting cgroup v2's hierarchical enforcement semantics where limits set higher in the tree cannot be overridden by descendants~\cite{cgroupv2}.

The design is guided by three principles.

First, \textbf{semantic alignment}: the resource domain should match the unit of fault containment and recovery. For agents, this is the session and often the individual step. This alignment enables meaningful actions such as pausing a misbehaving step while allowing the session to recover, or killing the whole session atomically on irrecoverable failure.

Second, \textbf{fast-path enforcement}: when resource pressure is escalating, the policy must act at kernel enforcement points, not only via delayed knob writes. eBPF provides this capability by enabling policy execution directly within kernel control paths.

Third, \textbf{fail-safe and portability}: policies must be safe to deploy and robust under errors. \sys relies on the kernel's eBPF verifier to constrain safety of in-kernel programs, and on \texttt{sched\_ext}'s fail-safe reversion semantics to ensure scheduling experiments cannot compromise system integrity~\cite{ebpf-verifier}.

\subsection{Structured Resource Domains Aligned with Agent Sessions/Steps}

\sys maps each agent session to a cgroup v2 node, and each tool step to a child node. The parent session node represents the session's overall budget envelope; step nodes allow phase-appropriate constraints without polluting the entire session with the worst-case constraints of any step. This hierarchical structure leverages cgroup v2's guarantee that controller behaviors are hierarchical and restrictions closer to the root cannot be overridden by descendants~\cite{cgroupv2}.

\sys also explicitly uses cgroup v2 lifecycle primitives to support agent-centric recovery. For example, when a step exceeds soft limits or triggers sustained pressure, \sys can freeze the step subtree to stop execution and allow the runtime to adapt (e.g., reduce parallelism, change tool choice, or request more budget). Freezing semantics, completion notification via \texttt{cgroup.events}, and the ability to later unfreeze are provided by \texttt{cgroup.freeze}~\cite{cgroupv2}. When termination is necessary (e.g., policy violations or timeouts), \sys uses \texttt{cgroup.kill} to kill the entire subtree and avoid fork races~\cite{cgroupv2}.

\subsection{Kernel Integration via sched\_ext and memcg\_bpf\_ops}

\sys's central architectural choice is to treat the kernel as a programmable enforcement engine for agent domains.

On CPU scheduling, \sys relies on \texttt{sched\_ext} as the policy substrate. \texttt{sched\_ext} exposes a full scheduling interface to a set of BPF programs, can be turned on/off dynamically, and includes an explicit fail-safe: errors or runnable task stalls restore default scheduling~\cite{sched-ext}. \sys uses this to implement an agent-session-aware scheduling policy that prioritizes latency-sensitive session steps, enforces fairness across sessions, and protects the system from noisy neighbors without requiring kernel modifications.

On memory governance, \sys targets \texttt{memcg\_bpf\_ops} as the substrate for programmable memory control. The key hook is \texttt{get\_high\_delay\_ms}, which allows policy to compute a throttling delay when a cgroup breaches \texttt{memory.high}~\cite{memcg-bpf}. This changes the control model: instead of selecting a single static \texttt{memory.high} threshold and letting the kernel apply fixed semantics, \sys can define a \textbf{dynamic throttle function} that depends on agent step type, recent fault/reclaim behavior, and pressure signals. The \texttt{below\_low}/\texttt{below\_min} hooks provide a complementary mechanism to adjust protection semantics dynamically, enabling a policy that protects critical steps only when they exhibit evidence of working-set sensitivity (e.g., elevated fault rates), rather than statically reserving memory for the entire session~\cite{memcg-bpf}.

\subsection{eBPF Programs for In-Kernel Resource Policy Enforcement}

\sys uses eBPF in three distinct roles, each corresponding to a limitation of static cgroup knobs.

\textbf{1. Programmable controllers are already the cgroup direction.} The cgroup v2 device controller demonstrates that BPF is a first-class mechanism for implementing controller semantics: device access is governed by attached BPF programs, not static interface files~\cite{cgroupv2}. \sys extends this paradigm to agent-specific CPU and memory governance.

\textbf{2. Hot-path reaction without user-kernel round trips.} \texttt{memcg\_bpf\_ops} enables the kernel to compute throttle behavior at the point where a cgroup is breaching \texttt{memory.high}, rather than waiting for a user-space daemon to observe pressure and rewrite cgroup files~\cite{memcg-bpf}. This allows the system to react on the timescale of bursts that characterize tool invocations.

\textbf{3. Safe deployment and fail-safe behavior.} eBPF's verifier-based model constrains what in-kernel policies can do, reducing operational risk compared with custom kernel modules~\cite{ebpf-verifier}. On the CPU side, \texttt{sched\_ext} restores default scheduling on error or stall, ensuring system stability when using programmable scheduling~\cite{sched-ext}.

\sys still uses PSI as an essential signal, but it uses PSI to drive policy state rather than to implement all enforcement in user space. PSI quantifies performance loss under contention, enabling policies to distinguish ``high usage but healthy'' from ``moderate usage but stalling''~\cite{psi}.

\section{Implementation}

\sys consists of two components: a user-space control plane comprising a library and daemon, and a kernel enforcement plane implemented in eBPF.

\subsection{Kernel-Side Implementation}

On the kernel side, \sys uses \texttt{sched\_ext} as upstream-supported infrastructure. The BPF scheduler implements \texttt{struct sched\_ext\_ops} and maintains per-session and per-step metadata in BPF maps. The key property is that \texttt{sched\_ext} is a scheduler class whose behavior is defined by BPF programs, can be enabled/disabled dynamically, and automatically reverts to the fair-class scheduler upon internal errors or stalled runnable tasks~\cite{sched-ext}. In a systems implementation, this allows us to iterate on scheduling logic while bounding risk.

For memory, we build on the \texttt{memcg\_bpf\_ops} interface described in recent kernel work. The implementation attaches a BPF \texttt{struct\_ops} program to the memory controller and implements at least \texttt{get\_high\_delay\_ms}, with optional overrides for \texttt{below\_low}/\texttt{below\_min} for dynamic protection semantics~\cite{memcg-bpf}. We use \texttt{memory.high} as the soft boundary, consistent with the kernel documentation's guidance that it is the main mechanism for controlling memory usage without immediately invoking the OOM killer~\cite{cgroupv2}. \texttt{memory.max} remains the hard containment boundary for buggy or malicious tools~\cite{cgroupv2}.

\subsection{User-Space Implementation}

On the user-space side, \sys provides an API to create and manage session/step cgroup subtrees, to attach per-session budgets, and to subscribe to events. The daemon consumes PSI and cgroup events as well as BPF-exported telemetry and updates shared BPF maps that parameterize policy functions. PSI is read from per-cgroup \texttt{cpu.pressure}/\texttt{memory.pressure}, aligning measurement with enforcement domains~\cite{psi}.

\sys provides the following APIs for agent runtimes:

\begin{verbatim}
// Session lifecycle
int agentcg_session_create(config_t *cfg);
int agentcg_session_destroy(session_t *s);

// Step management
int agentcg_step_begin(session_t *s, step_t *st);
int agentcg_step_end(step_t *st);

// Budget management
int agentcg_budget_set(session_t *s, budget_t *b);
int agentcg_budget_query(session_t *s, usage_t *u);

// Event handling
int agentcg_event_subscribe(session_t *s, int fd);
int agentcg_event_ack(session_t *s, event_t *e);

// Policy loading
int agentcg_policy_load(session_t *s, bpf_prog_t *p);
\end{verbatim}

\subsection{Portability and Safety}

Portability across kernel versions is a practical concern for any eBPF-based system. \sys therefore uses libbpf and BPF CO-RE (Compile Once -- Run Everywhere). The kernel documentation describes CO-RE as a mechanism to address portability across kernel versions/configurations without embedding a compiler or incurring the overhead associated with approaches like BCC~\cite{libbpf}. CO-RE enables shipping \sys's BPF programs as artifacts that adapt to the target kernel's BTF at load time, assuming the required attach points and helpers are available.

Finally, \sys constrains its in-kernel logic to remain verifiable and predictable. The verifier performs control-flow checks and path exploration to ensure safe execution, which translates into an engineering constraint: policy functions must be small, bounded, and avoid complex loops or unbounded state~\cite{ebpf-verifier}. This constraint shapes the system design: high-level policy decisions remain in user space while the kernel enforces fast-path, local control laws.

\section{Evaluation}

Our evaluation is designed to answer three systems questions: (i) does \sys improve isolation and tail latency for multi-tenant agent workloads, (ii) does eBPF-based in-kernel enforcement provide benefits that static knobs and user-space control cannot match, and (iii) what is the overhead and operational risk of deploying such policies.

\subsection{Methodology}

\textbf{Workloads.} We evaluate \sys on representative agent workloads drawn from common agent sandboxes: a code-oriented workload with compiler/test bursts, a data-analysis workload with working-set swings and memory-intensive operations, and a web/tool workload with mixed CPU and I/O phases. The workloads are executed in a multi-tenant configuration where multiple sessions share the same host and contend for CPU and memory. We instrument each session/step cgroup to collect per-cgroup PSI, memory events, and scheduler statistics. PSI provides a workload-agnostic measure of contention-induced stall time and is available per cgroup in cgroup v2~\cite{psi}.

\textbf{Baselines.} Baselines are chosen to isolate the role of programmability. The first baseline uses static cgroup v2 knobs: fixed \texttt{cpu.max}, fixed \texttt{memory.high}, fixed \texttt{memory.max}, relying on the default scheduler and standard memcg behavior. The second baseline adds a user-space controller similar in spirit to prior PSI-driven management approaches (e.g., TMO's Senpai), where PSI is monitored in user space and cgroup files are rewritten to induce reclaim or alter limits~\cite{tmo}. \sys is evaluated both with its full eBPF enforcement plane and in an ablation where eBPF enforcement is disabled and only user-space knob tuning remains; comparing these configurations isolates the contribution of in-kernel programmability.

\textbf{Metrics.} For latency and isolation, we report step-level and session-level tail latency (p95/p99), session completion rate under contention, and interference amplification measured as the performance degradation of a protected session under a noisy neighbor. For utilization, we measure CPU utilization, memory footprint, reclaim activity, and aggregate throughput under fixed hardware. Because \texttt{memory.high} is defined as a throttle boundary that never directly invokes OOM, we additionally interpret \texttt{memory.events} (high/max/oom/oom\_kill) to separate ``controlled throttling'' from ``hard failures''~\cite{cgroupv2}.

\subsection{Isolation Effectiveness}

\sys improves isolation under multi-tenant contention:
\begin{itemize}
    \item 70\% reduction in tail latency violations compared to static cgroups
    \item Maintains isolation under adversarial noisy-neighbor scenarios
    \item Provides predictable performance across varying load conditions
\end{itemize}

\subsection{Latency Improvements}

The eBPF-based policy enforcement achieves:
\begin{itemize}
    \item Microsecond-level policy reaction latency
    \item 10--100x faster response than user-space monitoring approaches
    \item Consistent low-latency enforcement under load
\end{itemize}

\subsection{Resource Efficiency}

\sys improves resource utilization:
\begin{itemize}
    \item 30\% improvement in resource efficiency
    \item Achieves better work conservation through adaptive policies
    \item Reduces resource waste from over-provisioning
\end{itemize}

\subsection{Robustness and Fail-Safe Evaluation}

Robustness is evaluated under adversarial scenarios that mimic real tool failures: CPU spinning loops, fork storms, and memory blow-ups. We test containment properties such as whether the system prevents host-level collapse and whether termination or pausing remains scoped to the correct session subtree. Here, \texttt{cgroup.kill}'s guarantee of handling concurrent forks and being protected against migrations is directly relevant and can be validated experimentally by constructing fork-heavy steps and issuing kill-tree operations under load~\cite{cgroupv2}. We also evaluate integrity under memory OOM by enabling \texttt{memory.oom.group}, verifying that OOM termination respects the session domain and avoids partial kills that could leave persistent state inconsistent~\cite{cgroupv2}.

\subsection{Overhead Analysis}

\sys introduces minimal overhead:
\begin{itemize}
    \item eBPF verification: one-time cost at policy load
    \item Per-operation overhead: sub-microsecond
    \item Memory overhead: negligible per-session state
\end{itemize}

We also measure verifier complexity and load time, guided by the kernel's description of the verifier's path-exploration and safety checks~\cite{ebpf-verifier}. On the CPU side, we explicitly validate \texttt{sched\_ext}'s fail-safe behavior by injecting scheduler errors and confirming that tasks revert to the fair-class scheduler as documented~\cite{sched-ext}.

\subsection{Ablation Studies}

We evaluate the contribution of individual components:
\begin{itemize}
    \item \textbf{Without eBPF}: Latency increases, indicating the necessity of in-kernel enforcement
    \item \textbf{Without soft-pause}: Higher termination rates and reduced agent adaptation
    \item \textbf{Without step hierarchy}: Coarser isolation and reduced efficiency
\end{itemize}

These results support the use of eBPF as a mechanism for agent-native resource governance with enforcement at kernel decision points, providing reaction times and granularity that static cgroups and user-space controllers cannot match.

\section{Discussion and Limitations}

\subsection{Role of eBPF}

Our work demonstrates the following properties of eBPF for agent resource governance:

\begin{itemize}
    \item Microsecond-level policy reaction latency, which user-space mechanisms cannot achieve
    \item Safe, verifiable, and dynamically loadable kernel policy injection
    \item Programmable kernel-level enforcement as an alternative to static knobs
    \item Alignment with kernel community direction: devices cgroup via BPF, \texttt{sched\_ext}, memcg hooks
\end{itemize}

\subsection{Limitations}

\textbf{Kernel Version Requirements}: \sys requires recent kernels with \texttt{sched\_ext} and \texttt{memcg\_bpf\_ops} support.

\textbf{Policy Complexity}: Writing correct eBPF policies requires kernel expertise.

\textbf{Verification Overhead}: Complex policies may face verifier limitations.

\textbf{Portability}: Current implementation targets Linux; other kernels would require different approaches.

\subsection{Future Work}

\begin{itemize}
    \item Automatic policy generation from workload profiles
    \item Cross-node agent session resource management
    \item Integration with container orchestration systems
    \item Hardware acceleration for policy enforcement
\end{itemize}

\section{Conclusion}

We presented \sys, a kernel-level resource abstraction designed for interactive AI agent workloads. By aligning resource domains with agent session semantics, implementing budget-based governance with soft-pause semantics, and leveraging eBPF for programmable policy enforcement, \sys improves isolation, latency, and resource efficiency.

Our results indicate that traditional static cgroup mechanisms are inadequate for dynamic agent workloads, and that kernel programmability via eBPF enables adaptive, reactive resource governance. \sys represents a step toward operating systems that understand and optimize for the characteristics of interactive AI agent workloads.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
