%%
%% This is file `main.tex',
%% AgentCgroup Paper
%%

\documentclass[sigconf,review,anonymous]{acmart}

%% Disable microtype to avoid font expansion issues
\PassOptionsToPackage{disable}{microtype}

%% System name macro
\usepackage{xspace}
\newcommand{\sys}{\mbox{AgentCgroup}\xspace}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

\begin{document}

\title{\sys: AI Agents Resource Control with eBPF}

\author{Anonymous Author(s)}
\affiliation{%
  \institution{Anonymous Institution}
  \city{Anonymous City}
  \country{Anonymous Country}
}
\email{anonymous@example.com}

\begin{abstract}
Interactive AI agents represent a distinct class of workloads characterized by short-lived, multi-stage executions with highly variable resource demands. Traditional approaches face two limitations: user-space controllers cannot react fast enough to millisecond-scale bursts, and static cgroup knobs lack the fine-grained control needed to adapt to dynamic workload phases.

We present \sys, an eBPF-based resource controller for agent workloads. \sys organizes resources around agent sessions and tool calls, and uses eBPF (sched\_ext for CPU scheduling, memcg\_bpf\_ops for memory throttling) to execute control logic at kernel enforcement points. This enables microsecond-level reaction and fine-grained decisions that user-space approaches cannot achieve.

Experimental evaluation demonstrates that \sys improves multi-tenant isolation and tail latency management while preventing critical resource contention scenarios with minimal overhead. Our work demonstrates the necessity of in-kernel enforcement for adaptive, reactive, and agent-aware resource governance.
\end{abstract}

% \begin{CCSXML}
% <ccs2012>
%    <concept>
%        <concept_id>10011007.10011006.10011008.10011009.10011015</concept_id>
%        <concept_desc>Software and its engineering~Operating systems</concept_desc>
%        <concept_significance>500</concept_significance>
%    </concept>
%    <concept>
%        <concept_id>10011007.10011006.10011008.10011024</concept_id>
%        <concept_desc>Software and its engineering~Language features</concept_desc>
%        <concept_significance>300</concept_significance>
%    </concept>
% </ccs2012>
% \end{CCSXML}

% \ccsdesc[500]{Software and its engineering~Operating systems}
% \ccsdesc[300]{Software and its engineering~Language features}

% \keywords{cgroup, eBPF, resource isolation, AI agents, operating systems}

\maketitle

\section{Introduction}

Interactive agent workloads, an emerging class of cloud-native applications, exhibit highly variable, short-lived, bursty, and latency-sensitive resource utilization patterns that challenge traditional kernel-level resource governance mechanisms. A single agent session alternates between reasoning phases, tool executions such as compilation or data processing, and interactive phases, each with distinct CPU and memory footprints. When multiple sessions share a host, a memory-intensive compilation burst in one session can trigger reclaim storms that inflate tail latency for neighbors, while fork-heavy tool invocations can exhaust CPU before static limits react. Linux cgroups, the de facto kernel abstraction for resource isolation, relies on static resource configurations and user-space reactive tuning. User-space controllers that react in tens of milliseconds cannot keep pace with millisecond-scale bursts, and static cgroup knobs cannot adapt to the varying resource needs of different execution phases.

In this paper, we present \sys, an eBPF-based resource controller for agent workloads. \sys organizes resources around agent sessions and tool calls, and uses eBPF (sched\_ext for CPU scheduling, memcg\_bpf\_ops for memory throttling) to execute control logic at kernel enforcement points.

\sys leverages eBPF to execute control logic directly at kernel enforcement points such as memory throttling paths and CPU scheduling hooks. This in-kernel execution enables microsecond-level reaction to resource pressure and fine-grained, workload-aware control that user-space approaches cannot achieve.

Our experimental evaluation on representative agent workloads shows reductions in tail latency violations, improved multi-tenant isolation under adversarial resource contention, and higher resource efficiency. This work demonstrates the value of in-kernel enforcement for responsive, adaptive, and workload-aware resource governance.

Key contributions:
\begin{itemize}
    \item Identifies two fundamental limitations of traditional approaches for agent workloads: slow user-space reaction and inflexible static cgroup knobs.
    \item Presents \sys, which uses eBPF to achieve fast reaction and fine-grained control at kernel enforcement points.
    \item Provides evaluation demonstrating improved isolation and latency under multi-tenant agent workloads.
\end{itemize}

\section{Background}

\textbf{Linux cgroup} provides a hierarchical resource governance abstraction where the kernel organizes tasks into a tree of control groups and applies controller-specific accounting and enforcement along that hierarchy~\cite{cgroupv2}. The memory controller exposes two key boundaries: \texttt{memory.high} as a soft throttle point that triggers reclaim pressure without invoking the OOM killer, and \texttt{memory.max} as a hard limit that triggers OOM when exceeded. Cgroup v2 also provides lifecycle controls relevant to agent execution: \texttt{cgroup.freeze} stops all processes in a subtree until explicitly unfrozen, \texttt{cgroup.kill} terminates all processes while handling concurrent forks, and \texttt{memory.oom.group} ensures OOM kills the entire cgroup atomically to avoid partial failures.

\textbf{eBPF} enables Linux to address the tension between standardized interfaces and dynamic workloads by introducing programmable enforcement points, providing a safe and dynamically loadable mechanism for executing control logic inside the kernel~\cite{ebpf-verifier}. On the CPU side, \texttt{sched\_ext} allows scheduling policies to be defined by BPF programs with fail-safe reversion to default behavior on errors~\cite{sched-ext}. On the memory side, \texttt{memcg\_bpf\_ops} introduces hooks such as \texttt{get\_high\_delay\_ms} for custom throttle delays on \texttt{memory.high} breaches~\cite{memcg-bpf}. \sys builds on these primitives to achieve fast reaction and fine-grained control for agent workloads.

\section{Motivation}

Traditional resource management for agent workloads faces two fundamental limitations.

The first limitation is timing mismatch. Agent workloads create short-lived, bursty subprocesses such as compilers, interpreters, and browser engines, with resource demands that change at millisecond timescales. User-space controllers that monitor pressure and rewrite cgroup files operate at 10-100ms timescales: by the time the controller observes memory pressure and adjusts limits, the burst has already caused reclaim storms or runqueue inflation. TMO demonstrates pressure-driven memory control at datacenter scale~\cite{tmo}, but its user-space Senpai component cannot react fast enough for agent bursts. This timing gap is fundamental: any approach based on user-space monitoring and cgroup file writes will be too slow.

The second limitation is granularity. Static cgroup knobs like \texttt{cpu.max} and \texttt{memory.max} provide fixed thresholds that cannot adapt to workload dynamics. A single agent session alternates between reasoning, tool execution, and post-processing phases with very different resource footprints~\cite{nalar}. A static budget applied uniformly is either conservative, wasting resources, or aggressive, causing failures during bursty phases. Mixture-of-Schedulers (ASA) shows that \texttt{sched\_ext} enables dynamic scheduling~\cite{asa}, and Nalar proposes two-level control for agent workflows~\cite{nalar}, but neither addresses the OS-level granularity gap: the kernel enforcement points themselves need to make fine-grained, workload-aware decisions rather than applying fixed limits.

These limitations motivate in-kernel enforcement: control logic must execute at kernel enforcement points to achieve microsecond-level reaction, and must make dynamic decisions based on workload state rather than applying static thresholds.

\section{Design and Implementation}

\subsection{Overview and Design Principles}

\sys defines an OS-level contract between an agent runtime and the kernel for resource governance. In this contract, the unit of governance is an agent session, optionally refined into tool calls. Each session and tool call is represented as a cgroup v2 subtree, inheriting cgroup v2's hierarchical enforcement semantics where limits set higher in the tree cannot be overridden by descendants.

The design addresses the limitations identified above through six principles: (1) domain alignment, where resource domains match agent session and tool-call boundaries; (2) fast-path enforcement, where policy executes in kernel control paths for microsecond-level reaction rather than user-space knob writes; (3) integrity semantics, providing atomic kill and cleanup to avoid partial session state; (4) graduated response, supporting throttling, soft-pause, and termination as escalating responses; (5) compatibility, building on existing cgroup v2 infrastructure without kernel modifications; and (6) safe deployment, leveraging eBPF verification~\cite{ebpf-verifier} and \texttt{sched\_ext}'s fail-safe reversion semantics~\cite{sched-ext}.

\subsection{Structured Resource Domains Aligned with Agent Sessions and Tool Calls}

\sys maps each agent session to a cgroup v2 node, and each tool call to a child node. The parent session node represents the session's overall budget envelope; tool-call nodes allow phase-appropriate constraints without polluting the entire session with the worst-case constraints of any single tool call. This hierarchical structure leverages cgroup v2's guarantee that controller behaviors are hierarchical and restrictions closer to the root cannot be overridden by descendants.

\sys also uses cgroup v2 lifecycle primitives to support agent-centric recovery. For example, when a tool call exceeds soft limits or triggers sustained pressure, \sys can freeze the tool-call subtree to stop execution and allow the runtime to adapt by reducing parallelism, changing tool choice, or requesting more budget. Freezing semantics, completion notification via \texttt{cgroup.events}, and the ability to later unfreeze are provided by \texttt{cgroup.freeze}. When termination is necessary due to policy violations or timeouts, \sys uses \texttt{cgroup.kill} to kill the entire subtree and avoid fork races.

\subsection{Kernel Enforcement via eBPF}

\sys uses eBPF to execute control logic directly at kernel enforcement points, enabling fast reaction without user-kernel round trips.

On CPU scheduling, \sys uses \texttt{sched\_ext}, which exposes a full scheduling interface to BPF programs~\cite{sched-ext}. The BPF scheduler implements \texttt{struct sched\_ext\_ops} and maintains per-session and per-tool-call metadata in BPF maps, prioritizing latency-sensitive tool calls and enforcing fairness across sessions. The scheduler can be enabled/disabled dynamically and automatically reverts to the fair-class scheduler on errors.

On memory governance, \sys uses \texttt{memcg\_bpf\_ops} hooks~\cite{memcg-bpf}. The implementation attaches a BPF \texttt{struct\_ops} program to the memory controller, implementing \texttt{get\_high\_delay\_ms} to compute dynamic throttling delays when a cgroup breaches \texttt{memory.high}. We use \texttt{memory.high} as the soft boundary and \texttt{memory.max} as the hard containment boundary~\cite{cgroupv2}.

The eBPF verifier constrains what in-kernel policies can do, reducing operational risk compared with kernel modules~\cite{ebpf-verifier}. For portability across kernel versions, \sys uses libbpf and BPF CO-RE~\cite{libbpf}.

\subsection{User-Space Control Plane}

The user-space component provides APIs to create and manage session and tool-call cgroup subtrees, attach per-session budgets, and subscribe to events. A daemon consumes cgroup events and BPF-exported telemetry and updates shared BPF maps that parameterize policy functions.

\section{Evaluation}

Our evaluation is designed to answer three systems questions: (i) does \sys improve isolation and tail latency for multi-tenant agent workloads, (ii) does eBPF-based in-kernel enforcement provide benefits that static knobs and user-space control cannot match, and (iii) what is the overhead and operational risk of deploying such policies.

\subsection{Methodology}

We evaluate \sys on representative agent workloads drawn from common agent sandboxes: a code-oriented workload with compiler/test bursts, a data-analysis workload with working-set swings and memory-intensive operations, and a web/tool workload with mixed CPU and I/O phases. The workloads are executed in a multi-tenant configuration where multiple sessions share the same host and contend for CPU and memory. We instrument each session and tool-call cgroup to collect memory events and scheduler statistics.

Baselines are chosen to isolate the benefit of in-kernel enforcement. The first baseline uses static cgroup v2 knobs: fixed \texttt{cpu.max}, fixed \texttt{memory.high}, fixed \texttt{memory.max}, relying on the default scheduler and standard memcg behavior. The second baseline adds a user-space controller similar to TMO's Senpai~\cite{tmo}, where memory pressure is monitored in user space and cgroup files are rewritten to induce reclaim or alter limits. \sys is evaluated both with its full eBPF enforcement plane and in an ablation where eBPF enforcement is disabled and only user-space knob tuning remains; comparing these configurations isolates the contribution of in-kernel enforcement.

For latency and isolation, we report tool-call-level and session-level tail latency (p95/p99), session completion rate under contention, and interference amplification measured as the performance degradation of a protected session under a noisy neighbor. For utilization, we measure CPU utilization, memory footprint, reclaim activity, and aggregate throughput under fixed hardware. Because \texttt{memory.high} is defined as a throttle boundary that never directly invokes OOM, we additionally interpret \texttt{memory.events} (high/max/oom/oom\_kill) to separate ``controlled throttling'' from ``hard failures''~\cite{cgroupv2}.

\subsection{Isolation, Latency, and Efficiency}

We measure isolation under multi-tenant contention by comparing tail latency violations across configurations, assessing whether \sys maintains isolation under adversarial noisy-neighbor scenarios. For latency, we compare policy reaction time between in-kernel enforcement and user-space monitoring, measuring the time from resource pressure detection to policy action. For efficiency, we compare aggregate throughput and memory footprint, assessing whether adaptive policies achieve better work conservation than static over-provisioning.

\subsection{Robustness and Fail-Safe Evaluation}

Robustness is evaluated under adversarial scenarios that mimic real tool failures: CPU spinning loops, fork storms, and memory blow-ups. We test containment properties such as whether the system prevents host-level collapse and whether termination or pausing remains scoped to the correct session subtree. The \texttt{cgroup.kill} guarantee of handling concurrent forks and protecting against migrations is directly relevant and can be validated experimentally by constructing fork-heavy tool calls and issuing kill-tree operations under load~\cite{cgroupv2}. We also evaluate integrity under memory OOM by enabling \texttt{memory.oom.group}, verifying that OOM termination respects the session domain and avoids partial kills that could leave persistent state inconsistent~\cite{cgroupv2}.

\subsection{Overhead Analysis}

We measure three categories of overhead: eBPF verification cost at policy load time, per-operation overhead on scheduling and memory control paths, and memory overhead for per-session BPF map state. We also measure verifier complexity and load time~\cite{ebpf-verifier}. On the CPU side, we validate \texttt{sched\_ext}'s fail-safe behavior by injecting scheduler errors and confirming that tasks revert to the fair-class scheduler~\cite{sched-ext}.

\section{Conclusion}

We presented \sys, an eBPF-based resource controller for interactive AI agent workloads. By executing control logic at kernel enforcement points via sched\_ext and memcg\_bpf\_ops, \sys achieves microsecond-level reaction and fine-grained control that user-space approaches cannot match.

Our results indicate that traditional approaches based on static cgroup configurations and user-space controllers are inadequate for agent workloads with millisecond-scale bursts. In-kernel enforcement via eBPF addresses this gap, enabling adaptive and responsive resource control for the emerging class of interactive AI agents.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
