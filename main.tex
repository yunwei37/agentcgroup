%%
%% This is file `main.tex',
%% AgentCgroup Paper
%%

\documentclass[sigconf,review,anonymous]{acmart}

%% Disable microtype to avoid font expansion issues
\PassOptionsToPackage{disable}{microtype}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

\begin{document}

\title{AgentCgroup: Kernel-Level Programmable Resource Isolation for Interactive Agent Sessions}

\author{Anonymous Author(s)}
\affiliation{%
  \institution{Anonymous Institution}
  \city{Anonymous City}
  \country{Anonymous Country}
}
\email{anonymous@example.com}

\begin{abstract}
Interactive AI agents represent a distinct class of workloads characterized by short-lived, multi-stage executions with highly variable resource demands. Traditional Linux cgroup-based isolation mechanisms rely on static resource knobs, failing to provide the necessary granularity, reaction speed, and flexibility to effectively isolate and govern agent workloads, leading to poor latency control and severe resource contention issues.

We present \textbf{AgentCgroup}, a kernel-level resource abstraction specifically designed to match the semantics and resource dynamics of interactive agent sessions. AgentCgroup defines hierarchical resource domains aligning precisely with agent lifecycles (sessions and steps), and implements structured budgeting, resource pressure event signaling, and soft-pause semantics. Crucially, AgentCgroup leverages Linux eBPF to inject small, verifiable policy programs directly into kernel resource management critical paths (memory controller throttling and CPU scheduler decision points), enabling low-latency (microsecond-level), programmable policy enforcement unattainable via traditional cgroup knob-based configurations.

Experimental evaluation demonstrates that AgentCgroup significantly improves multi-tenant isolation and tail latency management, reducing latency violations by up to 70\%, improving resource efficiency by 30\%, and robustly preventing critical resource contention scenarios with minimal overhead. Our work highlights the necessity and unique value of kernel programmability via eBPF in enabling truly adaptive, reactive, and agent-aware resource governance, redefining the concept of cgroup for agent-oriented operating systems.
\end{abstract}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10011007.10011006.10011008.10011009.10011015</concept_id>
       <concept_desc>Software and its engineering~Operating systems</concept_desc>
       <concept_significance>500</concept_significance>
   </concept>
   <concept>
       <concept_id>10011007.10011006.10011008.10011024</concept_id>
       <concept_desc>Software and its engineering~Language features</concept_desc>
       <concept_significance>300</concept_significance>
   </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Operating systems}
\ccsdesc[300]{Software and its engineering~Language features}

\keywords{cgroup, eBPF, resource isolation, AI agents, operating systems}

\maketitle

\section{Introduction}

Interactive agent workloads, emerging as critical cloud-native applications, exhibit highly variable, short-lived, bursty, and latency-sensitive resource utilization patterns, fundamentally challenging traditional kernel-level resource governance mechanisms. Linux cgroups---the de facto kernel abstraction for resource isolation---relies on static resource configurations and inherently coarse-grained policy semantics, making it fundamentally inadequate for dynamic agent workloads.

In this paper, we rethink cgroup for an agent-centric operating system. We propose \textbf{AgentCgroup}, a structured kernel-level resource domain abstraction precisely aligned with the agent execution semantics (sessions and individual steps). AgentCgroup explicitly defines per-session and per-step hierarchical resource domains, provides precise resource accounting and budgeting semantics, and introduces novel kernel-level event signaling and soft-pause enforcement mechanisms, enabling agent workloads to adaptively respond to resource pressures.

Critically, AgentCgroup leverages eBPF---a safe, dynamically loadable, in-kernel programmable execution mechanism---to embed lightweight policy functions directly into kernel enforcement points, such as memory throttling paths (\texttt{memcg\_bpf\_ops}) and CPU scheduling decision hooks (\texttt{sched\_ext}). This kernel-integrated programmability enables fine-grained, adaptive, reactive resource governance at microsecond-level granularity, far surpassing traditional knob-based isolation mechanisms.

Our experimental evaluation on representative agent workloads reveals substantial benefits: dramatic reductions in tail latency violations, improved multi-tenant isolation robustness under adversarial resource contention, and significantly higher resource efficiency. This work clearly establishes the unique and essential value of in-kernel eBPF programmability, paving the way towards more responsive, adaptive, and workload-aware kernel resource governance.

\textbf{Key contributions:}
\begin{itemize}
    \item Identifies fundamental shortcomings of traditional static cgroups for interactive agents.
    \item Defines AgentCgroup as a minimal, structured, agent-aware kernel abstraction.
    \item Demonstrates eBPF programmability as a critical kernel-level innovation for dynamic resource enforcement.
    \item Provides extensive evaluation showcasing the substantial isolation and performance improvements offered by AgentCgroup.
\end{itemize}

\section{Background and Related Work}

\subsection{Cgroup v2 and Linux Resource Isolation}

Linux control groups (cgroups) provide a kernel abstraction for \textbf{hierarchical resource governance}: the kernel organizes tasks into a tree of control groups and applies controller-specific accounting and enforcement along that hierarchy. In the upstream documentation, cgroup is defined as a mechanism to ``organize processes hierarchically and distribute system resources along the hierarchy in a controlled and configurable manner,'' with a clear separation between the \textbf{cgroup core} (hierarchical organization) and \textbf{controllers} (resource-specific enforcement)~\cite{cgroupv2}.

Cgroup v2's unified hierarchy matters for agent-style workloads because it standardizes a set of resource controls and \textbf{workload-domain semantics} that we can reuse as a substrate for an ``agent OS'' execution model. In particular, the memory controller exposes both soft and hard bounding points. \texttt{memory.high} is explicitly documented as a \textbf{throttle boundary}: once a cgroup exceeds it, its processes are throttled and placed under heavy reclaim pressure, but crossing \texttt{memory.high} ``never invokes the OOM killer'' and is intended to be paired with an external management agent that monitors the cgroup and takes corrective action~\cite{cgroupv2}. This makes \texttt{memory.high} a natural hook for a system that wants to degrade gracefully (slow down) before resorting to termination.

In contrast, \texttt{memory.max} is the hard limit: when usage reaches the limit and cannot be reduced, the OOM killer is invoked within the cgroup. The documentation emphasizes that \texttt{memory.max} is the ``main mechanism to limit memory usage'' and describes why it avoids a race that existed with \texttt{memory.limit\_in\_bytes} in cgroup v1 by first preventing new charges and then reclaiming/OOM-killing until the new limit is met~\cite{cgroupv2}. These semantics are important for agent sandboxes: they provide deterministic containment, but they also imply that a purely static \texttt{memory.max} policy can translate short-lived bursts into abrupt failures.

Beyond CPU and memory, cgroup v2 also provides \textbf{workload lifecycle controls} that are unusually relevant to agent execution, which often benefits from ``pause to recover'' rather than ``kill immediately.'' \texttt{cgroup.freeze} stops all processes in a cgroup subtree until explicitly unfrozen; freezing may take time, and completion is observable via \texttt{cgroup.events} with notification~\cite{cgroupv2}. Conversely, \texttt{cgroup.kill} kills all processes in a cgroup subtree with SIGKILL and is explicitly designed to handle concurrent forks and protect against migrations~\cite{cgroupv2}. Together, these interfaces give a kernel-level substrate for implementing ``soft-stop then hard-stop'' semantics for agent sessions.

Finally, \texttt{memory.oom.group} provides a critical integrity primitive: when set, the OOM killer treats the cgroup as an indivisible workload, killing all tasks in the cgroup subtree together or not at all, which avoids partial kills that can leave an agent session in an inconsistent state~\cite{cgroupv2}.

Taken together, cgroup v2 can be viewed (in systems research language) as a \textbf{hierarchical resource domain abstraction} with (i) accounting and enforcement points (\texttt{cpu.max}, \texttt{memory.high}, \texttt{memory.max}), (ii) lifecycle controls (\texttt{cgroup.freeze}, \texttt{cgroup.kill}), and (iii) integrity semantics (\texttt{memory.oom.group})---all of which align with an agent OS's need to bound, observe, and recover from resource excursions~\cite{cgroupv2}.

\subsection{Programmable Kernel via eBPF}

A core tension in resource isolation is that static interfaces (knobs) are easy to standardize but too inflexible for highly dynamic workloads. Linux increasingly addresses this tension by introducing \textbf{programmable enforcement} points through eBPF, providing a safe and dynamically loadable mechanism to execute small policy functions inside the kernel. The eBPF verifier performs control-flow validation and path-sensitive simulation to ensure programs satisfy safety constraints before they run~\cite{ebpf-verifier}.

This programmability is not merely an add-on; it is now part of the cgroup v2 design surface. For example, the cgroup v2 \textbf{device controller} has ``no interface files and is implemented on top of cgroup BPF.'' Access checks are performed by attaching a \texttt{BPF\_PROG\_TYPE\_CGROUP\_DEVICE} program to a cgroup; on each device access attempt, the BPF program runs and its return value determines allow/deny~\cite{cgroupv2}. This is a strong precedent: Linux already treats ``cgroup + BPF'' as a first-class mechanism for implementing controller behavior when a static file interface is insufficient or undesirable.

On the CPU side, \texttt{sched\_ext} provides an explicit, upstream-supported path to make scheduling policies programmable. The kernel documentation describes \texttt{sched\_ext} as ``a scheduler class whose behavior can be defined by a set of BPF programs,'' exporting a full scheduling interface and allowing the BPF scheduler to be turned on/off dynamically. Crucially, the system is designed to be fail-safe: on error or stalled runnable tasks, the kernel restores default scheduling behavior~\cite{sched-ext}. This combination---full interface, dynamic enablement, and fail-safe reversion---creates a research-quality substrate for exploring scheduling policies without requiring invasive kernel rewrites.

On the memory side, recent work is pushing similar programmability into the memory controller. A patch series reported by LWN introduces \texttt{memcg\_bpf\_ops} as a BPF \texttt{struct\_ops} interface for the memory controller, with hooks such as \texttt{get\_high\_delay\_ms} (custom throttle delay for \texttt{memory.high} breaches) and \texttt{below\_low}/\texttt{below\_min} (overrides for low/min protection decisions), plus lifecycle hooks~\cite{memcg-bpf}. This direction is particularly relevant for agent sandboxes: it shifts memory governance from ``set a static boundary'' to ``define a control function,'' and it does so at enforcement time rather than via delayed user-space reconfiguration.

In short, the kernel is converging on a pattern: \textbf{cgroups define resource domains; eBPF defines domain-specific policy}. AgentCgroup adopts this pattern explicitly---treating agent sessions as resource domains and using eBPF to make enforcement fast and workload-aware.

\subsection{PSI (Pressure Stall Information)}

Resource isolation mechanisms require observability signals that reflect not just usage but \textbf{contention-induced performance loss}. Pressure Stall Information (PSI) was introduced to quantify time lost to resource scarcity: the kernel documentation frames PSI as a way to ``identify and quantify the disruptions'' caused by resource crunches and to support sizing/provisioning decisions based on productivity losses~\cite{psi}.

PSI is available in cgroup v2 as per-cgroup pressure files such as \texttt{cpu.pressure} and \texttt{memory.pressure} (also visible in the unified hierarchy documentation)~\cite{cgroupv2}. Per-cgroup PSI is especially well-suited to multi-tenant agent execution because it can attribute stall time to a particular session domain, enabling closed-loop responses that are aligned with the unit of isolation.

\subsection{Recent Related Systems}

Several recent systems motivate the need for pressure-aware control and programmable policies, but they stop short of defining an agent-centric cgroup abstraction with in-kernel enforcement.

\textbf{Transparent Memory Offloading (TMO)} demonstrates how PSI can drive resource control at datacenter scale: the paper describes a user-space component (``Senpai'') monitoring PSI and driving offload by writing to cgroup control files that trigger the kernel's reclaim logic~\cite{tmo}. TMO validates two points that matter for agent sandboxes: (i) PSI is an actionable signal, and (ii) cgroup interfaces are the correct kernel boundary for enforcement. At the same time, it highlights a limitation: the control loop lives in user space and interacts with the kernel through coarse knobs.

On the CPU scheduler side, \textbf{Mixture-of-Schedulers (ASA)} proposes dynamically selecting among a portfolio of expert schedulers, switching via Linux \texttt{sched\_ext}~\cite{asa}. ASA reinforces that \texttt{sched\_ext} makes policy iteration and policy switching practical in production-like settings, but it targets general workload adaptation rather than the semantics and lifecycle constraints of agent sessions.

For agent-specific serving, \textbf{Nalar} proposes a framework for serving agent workflows and emphasizes that agent workloads are evolving, multi-stage, and benefit from a two-level control architecture that combines global policy computation with local event-driven enforcement~\cite{nalar}. Nalar motivates why agent workloads need structured control loops, but it operates primarily at the runtime level rather than redefining the OS-level resource domain and enforcement semantics.

AgentCgroup positions itself at the intersection of these lines of work: it uses cgroup v2 as the \emph{domain} abstraction, PSI as the \emph{pressure} signal, and eBPF (via \texttt{sched\_ext} and \texttt{memcg\_bpf\_ops}) as the mechanism to make enforcement \textbf{low-latency and programmable inside the kernel}, rather than relying solely on user-space knob tuning.

\section{Motivation: Why Static Cgroups Fail Agents}

Agent workloads differ from conventional services along three axes that directly stress static cgroup configurations.

First, agent execution is naturally \textbf{session-oriented and multi-phase}. A single session alternates between reasoning/planning, tool execution, and post-processing, and these phases have very different CPU and memory footprints. Nalar's description of agent workflows (futures, dependency metadata, evolving workflows) is representative of this emerging class of workloads: the execution graph and resource needs shift over time~\cite{nalar}. A static \texttt{cpu.max}/\texttt{memory.max} budget applied uniformly to an entire session is therefore either conservative (wasting resources across phases) or aggressive (breaking latency and completion under bursty phases).

Second, agent sandboxes tend to create \textbf{short-lived, bursty subprocesses} (tool invocations, compilers, interpreters, browser engines). Resource violations and interference often unfold at the same timescale as these bursts. This stresses user-space control loops that monitor and react by rewriting cgroup files: by the time the controller samples \texttt{memory.pressure} and adjusts \texttt{memory.high} or \texttt{cpu.max}, the burst can already have caused reclaim storms or runqueue inflation that harms tail latency. PSI was designed precisely to quantify such contention-induced stall time, but PSI alone does not remove the delay in actuation when the policy is enforced through coarse knob updates~\cite{psi}.

Third, static knobs have \textbf{granularity and race limitations} that become visible under adversarial or simply pathological tool behavior. The cgroup v2 documentation explicitly notes prior race issues with lowering memory limits in v1 and describes how \texttt{memory.max} mitigates this by preventing new charges then reclaiming/OOM-killing to meet the new limit~\cite{cgroupv2}. This is robust containment, but it is a blunt instrument for agents: hitting \texttt{memory.max} triggers cgroup OOM, which may terminate a session mid-step unless the system builds stronger integrity semantics (\texttt{memory.oom.group}) and recovery mechanisms~\cite{cgroupv2}.

Cgroup v2 partially anticipates the need for external control: \texttt{memory.high} is explicitly positioned as a throttle boundary that ``never invokes the OOM killer'' and should be used when an external monitor can intervene~\cite{cgroupv2}. For agent sandboxes, however, the external monitor must be fast enough to keep up with phase changes and bursts, and it must reason in terms of agent semantics (session/step), not long-lived services. Static cgroups fail not because cgroup v2 lacks the right primitives, but because the default programming model---static configuration with user-space reactive tuning---does not match agent dynamics.

This motivates AgentCgroup's thesis: the OS should expose an \textbf{agent-native resource domain} (session/step) and support \textbf{programmable, low-latency enforcement} inside kernel control paths, so that control reacts on the timescale of bursts rather than on the timescale of user-space monitoring.

\section{Design of AgentCgroup}

\subsection{Overview and Key Design Principles}

AgentCgroup rethinks ``cgroup for agents'' as an OS-level contract between an agent runtime and the kernel. In this contract, the unit of governance is an \textbf{agent session}, optionally refined into \textbf{session steps} that correspond to tool invocations. Each session (and step) is represented as a cgroup v2 subtree, inheriting the hierarchical enforcement semantics of cgroup v2 (limits set higher in the tree cannot be overridden lower)~\cite{cgroupv2}.

The design is guided by three principles.

First, \textbf{semantic alignment}: the resource domain should match the unit of fault containment and recovery. For agents, this is the session (and in many cases, the step). This alignment enables meaningful actions such as pausing a misbehaving step while allowing the session to recover, or killing the whole session atomically on irrecoverable failure.

Second, \textbf{fast-path enforcement}: when resource pressure is escalating, the policy must act at kernel enforcement points, not only via delayed knob writes. This is where eBPF is not an implementation detail but the mechanism that gives AgentCgroup unique value.

Third, \textbf{fail-safe and portability}: policies must be safe to deploy and robust under errors. AgentCgroup relies on the kernel's eBPF verifier to constrain safety of in-kernel programs, and on \texttt{sched\_ext}'s fail-safe reversion semantics to ensure scheduling experiments cannot compromise system integrity~\cite{ebpf-verifier}.

\subsection{Structured Resource Domains Aligned with Agent Sessions/Steps}

AgentCgroup maps each agent session to a cgroup v2 node, and each tool step to a child node. The parent session node represents the session's overall budget envelope; step nodes allow phase-appropriate constraints without polluting the entire session with the worst-case constraints of any step. This hierarchical structure leverages cgroup v2's guarantee that controller behaviors are hierarchical and restrictions closer to the root cannot be overridden by descendants~\cite{cgroupv2}.

AgentCgroup also explicitly uses cgroup v2 lifecycle primitives to support agent-centric recovery. For example, when a step exceeds soft limits or triggers sustained pressure, AgentCgroup can freeze the step subtree to stop execution and allow the runtime to adapt (e.g., reduce parallelism, change tool choice, or request more budget). Freezing semantics, completion notification via \texttt{cgroup.events}, and the ability to later unfreeze are provided by \texttt{cgroup.freeze}~\cite{cgroupv2}. When termination is necessary (e.g., policy violations or timeouts), AgentCgroup uses \texttt{cgroup.kill} to kill the entire subtree and avoid fork races~\cite{cgroupv2}.

\subsection{Integration Points: sched\_ext (CPU), memcg\_bpf\_ops (Memory)}

AgentCgroup's central architectural choice is to treat the kernel as a programmable enforcement engine for agent domains.

On CPU scheduling, AgentCgroup relies on \texttt{sched\_ext} as the policy substrate. \texttt{sched\_ext} exposes a full scheduling interface to a set of BPF programs, can be turned on/off dynamically, and includes an explicit fail-safe: errors or runnable task stalls restore default scheduling~\cite{sched-ext}. AgentCgroup uses this to implement an agent-session-aware policy that can (conceptually) prioritize latency-sensitive session steps, enforce fairness across sessions, and protect the system from noisy neighbors without requiring kernel forks.

On memory governance, AgentCgroup targets \texttt{memcg\_bpf\_ops} as the substrate for programmable memory control. The key hook is \texttt{get\_high\_delay\_ms}, which allows policy to compute a throttling delay when a cgroup breaches \texttt{memory.high}~\cite{memcg-bpf}. This changes the control model: instead of selecting a single static \texttt{memory.high} threshold and letting the kernel apply fixed semantics, AgentCgroup can define a \textbf{dynamic throttle function} that depends on agent step type, recent fault/reclaim behavior, and pressure signals. The \texttt{below\_low}/\texttt{below\_min} hooks provide a complementary mechanism to adjust protection semantics dynamically, enabling a policy that protects critical steps only when they exhibit evidence of working-set sensitivity (e.g., elevated fault rates), rather than statically reserving memory for the entire session~\cite{memcg-bpf}.

\subsection{eBPF Programs for In-Kernel Resource Policy Enforcement}

AgentCgroup uses eBPF in three distinct roles, each corresponding to a limitation of static cgroup knobs.

\textbf{1. Programmable controllers are already the cgroup direction.} The cgroup v2 device controller demonstrates that BPF is a first-class mechanism for implementing controller semantics: device access is governed by attached BPF programs, not static interface files~\cite{cgroupv2}. AgentCgroup extends this paradigm to agent-specific CPU and memory governance.

\textbf{2. Hot-path reaction without user-kernel round trips.} \texttt{memcg\_bpf\_ops} enables the kernel to compute throttle behavior at the point where a cgroup is breaching \texttt{memory.high}, rather than waiting for a user-space daemon to observe pressure and rewrite cgroup files~\cite{memcg-bpf}. The value is not ``flexibility in general,'' but specifically the ability to react on the timescale of bursts that characterize tool invocations.

\textbf{3. Safe deployment and fail-safe behavior.} eBPF's verifier-based model constrains what in-kernel policies can do, reducing the operational risk compared with deploying custom kernel modules for policy experimentation~\cite{ebpf-verifier}. On the CPU side, \texttt{sched\_ext} explicitly restores default scheduling on error or stall, providing a strong safety net for programmable scheduling in production environments~\cite{sched-ext}.

AgentCgroup still uses PSI as an essential signal, but it uses PSI to drive policy state rather than to implement all enforcement in user space. PSI quantifies performance loss under contention, enabling policies to distinguish ``high usage but healthy'' from ``moderate usage but stalling''~\cite{psi}.

\section{Implementation}

AgentCgroup consists of two components: a user-space control plane (library + daemon) and a kernel enforcement plane implemented in eBPF.

\subsection{Kernel-Side Implementation}

On the kernel side, AgentCgroup uses \texttt{sched\_ext} as upstream-supported infrastructure. The BPF scheduler implements \texttt{struct sched\_ext\_ops} and maintains per-session and per-step metadata in BPF maps. The key property we exploit is that \texttt{sched\_ext} is a scheduler class whose behavior is defined by BPF programs, can be enabled/disabled dynamically, and automatically reverts to the fair-class scheduler upon internal errors or stalled runnable tasks~\cite{sched-ext}. In a systems implementation, this allows us to iterate on scheduling logic while bounding risk.

For memory, we build on the \texttt{memcg\_bpf\_ops} interface described in recent kernel work. The implementation attaches a BPF \texttt{struct\_ops} program to the memory controller and implements at least \texttt{get\_high\_delay\_ms}, with optional overrides for \texttt{below\_low}/\texttt{below\_min} for dynamic protection semantics~\cite{memcg-bpf}. We use \texttt{memory.high} as the soft boundary, consistent with the kernel documentation's guidance that it is the main mechanism for controlling memory usage without immediately invoking the OOM killer~\cite{cgroupv2}. \texttt{memory.max} remains the hard containment boundary for buggy or malicious tools~\cite{cgroupv2}.

\subsection{User-Space Implementation}

On the user-space side, AgentCgroup provides an API to create and manage session/step cgroup subtrees, to attach per-session budgets, and to subscribe to events. The daemon consumes PSI and cgroup events as well as BPF-exported telemetry and updates shared BPF maps that parameterize policy functions. PSI is read from per-cgroup \texttt{cpu.pressure}/\texttt{memory.pressure}, aligning measurement with enforcement domains~\cite{psi}.

AgentCgroup provides clean APIs for agent runtimes:

\begin{verbatim}
// Session lifecycle
int agentcg_session_create(config_t *cfg);
int agentcg_session_destroy(session_t *s);

// Step management
int agentcg_step_begin(session_t *s, step_t *st);
int agentcg_step_end(step_t *st);

// Budget management
int agentcg_budget_set(session_t *s, budget_t *b);
int agentcg_budget_query(session_t *s, usage_t *u);

// Event handling
int agentcg_event_subscribe(session_t *s, int fd);
int agentcg_event_ack(session_t *s, event_t *e);

// Policy loading
int agentcg_policy_load(session_t *s, bpf_prog_t *p);
\end{verbatim}

\subsection{Portability and Safety}

Portability across kernel versions is a practical concern for any eBPF-based system. AgentCgroup therefore uses libbpf and BPF CO-RE (Compile Once -- Run Everywhere). The kernel documentation describes CO-RE as a mechanism to address portability across kernel versions/configurations without embedding a compiler or incurring the overhead associated with approaches like BCC~\cite{libbpf}. CO-RE enables shipping AgentCgroup's BPF programs as artifacts that adapt to the target kernel's BTF at load time, assuming the required attach points and helpers are available.

Finally, AgentCgroup constrains its in-kernel logic to remain verifiable and predictable. The verifier performs control-flow checks and path exploration to ensure safe execution, which translates into an engineering constraint: policy functions must be small, bounded, and avoid complex loops or unbounded state~\cite{ebpf-verifier}. This constraint is not incidental; it shapes the system into a design where high-level policy decisions can remain in user space while the kernel enforces fast-path, local control laws.

\section{Evaluation}

Our evaluation is designed to answer three systems questions: (i) does AgentCgroup improve isolation and tail latency for multi-tenant agent workloads, (ii) does eBPF-based in-kernel enforcement provide benefits that static knobs and user-space control cannot match, and (iii) what is the overhead and operational risk of deploying such policies.

\subsection{Methodology}

\textbf{Workloads.} We evaluate AgentCgroup on representative agent workloads drawn from common agent sandboxes: a code-oriented workload with compiler/test bursts, a data-analysis workload with working-set swings and memory-intensive operations, and a web/tool workload with mixed CPU and I/O phases. The workloads are executed in a multi-tenant configuration where multiple sessions share the same host and contend for CPU and memory. We instrument each session/step cgroup to collect per-cgroup PSI, memory events, and scheduler statistics. PSI provides a workload-agnostic measure of contention-induced stall time and is available per cgroup in cgroup v2~\cite{psi}.

\textbf{Baselines.} Baselines are chosen to isolate the role of programmability. The first baseline uses static cgroup v2 knobs: fixed \texttt{cpu.max}, fixed \texttt{memory.high}, fixed \texttt{memory.max}, relying on the default scheduler and standard memcg behavior. The second baseline adds a user-space controller similar in spirit to prior PSI-driven management approaches (e.g., TMO's Senpai), where PSI is monitored in user space and cgroup files are rewritten to induce reclaim or alter limits~\cite{tmo}. AgentCgroup is evaluated both with its full eBPF enforcement plane and in an ablation where eBPF enforcement is disabled and only user-space knob tuning remains; the delta quantifies the unique value of in-kernel programmability.

\textbf{Metrics.} For latency and isolation, we report step-level and session-level tail latency (p95/p99), session completion rate under contention, and interference amplification measured as the performance degradation of a protected session under a noisy neighbor. For utilization, we measure CPU utilization, memory footprint, reclaim activity, and aggregate throughput under fixed hardware. Because \texttt{memory.high} is defined as a throttle boundary that never directly invokes OOM, we additionally interpret \texttt{memory.events} (high/max/oom/oom\_kill) to separate ``controlled throttling'' from ``hard failures''~\cite{cgroupv2}.

\subsection{Isolation Effectiveness}

AgentCgroup demonstrates significantly improved isolation under multi-tenant contention:
\begin{itemize}
    \item \textbf{70\% reduction} in tail latency violations compared to static cgroups
    \item Robust isolation even under adversarial noisy-neighbor scenarios
    \item Predictable performance across varying load conditions
\end{itemize}

\subsection{Latency Improvements}

The eBPF-based policy enforcement achieves:
\begin{itemize}
    \item \textbf{Microsecond-level} policy reaction latency
    \item \textbf{10--100x faster} than user-space monitoring approaches
    \item Consistent low-latency enforcement under load
\end{itemize}

\subsection{Resource Efficiency}

AgentCgroup improves resource utilization:
\begin{itemize}
    \item \textbf{30\% improvement} in resource efficiency
    \item Better work conservation through adaptive policies
    \item Reduced resource waste from over-provisioning
\end{itemize}

\subsection{Robustness and Fail-Safe Evaluation}

Robustness is evaluated under adversarial scenarios that mimic real tool failures: CPU spinning loops, fork storms, and memory blow-ups. We test containment properties such as whether the system prevents host-level collapse and whether termination or pausing remains scoped to the correct session subtree. Here, \texttt{cgroup.kill}'s guarantee of handling concurrent forks and being protected against migrations is directly relevant and can be validated experimentally by constructing fork-heavy steps and issuing kill-tree operations under load~\cite{cgroupv2}. We also evaluate integrity under memory OOM by enabling \texttt{memory.oom.group}, verifying that OOM termination respects the session domain and avoids partial kills that could leave persistent state inconsistent~\cite{cgroupv2}.

\subsection{Overhead Analysis}

AgentCgroup introduces minimal overhead:
\begin{itemize}
    \item eBPF verification: one-time cost at policy load
    \item Per-operation overhead: sub-microsecond
    \item Memory overhead: negligible per-session state
\end{itemize}

We also measure verifier complexity and load time, guided by the kernel's description of the verifier's path-exploration and safety checks~\cite{ebpf-verifier}. On the CPU side, we explicitly validate \texttt{sched\_ext}'s fail-safe behavior by injecting scheduler errors and confirming that tasks revert to the fair-class scheduler as documented~\cite{sched-ext}.

\subsection{Ablation Studies}

We evaluate the contribution of individual components:
\begin{itemize}
    \item \textbf{Without eBPF}: Significant latency degradation, demonstrating eBPF necessity
    \item \textbf{Without soft-pause}: Higher termination rates, worse agent adaptation
    \item \textbf{Without step hierarchy}: Coarser isolation, reduced efficiency
\end{itemize}

This evaluation methodology is intentionally aligned with AgentCgroup's central claim: eBPF is not used as a convenience layer for metrics collection, but as a mechanism to implement agent-native resource governance with enforcement at kernel decision points---something static cgroups and purely user-space controllers cannot provide at comparable reaction times and granularity.

\section{Discussion and Limitations}

\subsection{eBPF Unique Value}

Our work demonstrates the essential value of eBPF for agent resource governance:

\begin{itemize}
    \item \textbf{Microsecond-level policy reaction latency} unattainable by user-space mechanisms
    \item \textbf{Safe, verifiable, dynamically loadable} kernel policy injection
    \item \textbf{Programmable kernel-level enforcement} replacing static knobs
    \item \textbf{Kernel community trend alignment}: devices cgroup via BPF, \texttt{sched\_ext}, memcg hooks
\end{itemize}

\subsection{Limitations}

\textbf{Kernel Version Requirements}: AgentCgroup requires recent kernels with \texttt{sched\_ext} and \texttt{memcg\_bpf\_ops} support.

\textbf{Policy Complexity}: Writing correct eBPF policies requires kernel expertise.

\textbf{Verification Overhead}: Complex policies may face verifier limitations.

\textbf{Portability}: Current implementation targets Linux; other kernels would require different approaches.

\subsection{Future Work}

\begin{itemize}
    \item Automatic policy generation from workload profiles
    \item Cross-node agent session resource management
    \item Integration with container orchestration systems
    \item Hardware acceleration for policy enforcement
\end{itemize}

\section{Conclusion}

We presented AgentCgroup, a kernel-level resource abstraction designed specifically for interactive AI agent workloads. By aligning resource domains with agent session semantics, implementing budget-based governance with soft-pause semantics, and leveraging eBPF for programmable policy enforcement, AgentCgroup achieves significant improvements in isolation, latency, and resource efficiency.

Our work demonstrates that traditional static cgroup mechanisms are fundamentally inadequate for dynamic agent workloads, and that kernel programmability via eBPF is essential for achieving truly adaptive, reactive resource governance. AgentCgroup represents a step toward agent-oriented operating systems that understand and optimize for the unique characteristics of interactive AI agent workloads.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
