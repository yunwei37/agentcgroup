%%
%% This is file `main.tex',
%% AgentCgroup Paper
%%

\documentclass[sigconf,review,anonymous]{acmart}

%% Disable microtype to avoid font expansion issues
\PassOptionsToPackage{disable}{microtype}

%% System name macro
\usepackage{xspace}
\newcommand{\sys}{\mbox{AgentCgroup}\xspace}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

\begin{document}

\title{\sys: Kernel-Level Programmable Resource Isolation for Interactive Agent Sessions}

\author{Anonymous Author(s)}
\affiliation{%
  \institution{Anonymous Institution}
  \city{Anonymous City}
  \country{Anonymous Country}
}
\email{anonymous@example.com}

\begin{abstract}
Interactive AI agents represent a distinct class of workloads characterized by short-lived, multi-stage executions with highly variable resource demands. Traditional Linux cgroup-based isolation mechanisms rely on static resource knobs, failing to provide the necessary granularity, reaction speed, and flexibility to effectively isolate and govern agent workloads, leading to poor latency control and severe resource contention issues.

We present \sys, a kernel-level resource abstraction specifically designed to match the semantics and resource dynamics of interactive agent sessions. \sys defines hierarchical resource domains aligning precisely with agent lifecycles, including sessions and steps, and implements structured budgeting, resource pressure event signaling, and soft-pause semantics. \sys leverages Linux eBPF to inject small, verifiable policy programs directly into kernel resource management paths such as memory controller throttling and CPU scheduler decision points, enabling microsecond-level programmable policy enforcement that traditional cgroup configurations cannot provide.

Experimental evaluation demonstrates that \sys improves multi-tenant isolation and tail latency management while preventing critical resource contention scenarios with minimal overhead. Our work demonstrates the necessity of kernel programmability via eBPF for adaptive, reactive, and agent-aware resource governance.
\end{abstract}

% \begin{CCSXML}
% <ccs2012>
%    <concept>
%        <concept_id>10011007.10011006.10011008.10011009.10011015</concept_id>
%        <concept_desc>Software and its engineering~Operating systems</concept_desc>
%        <concept_significance>500</concept_significance>
%    </concept>
%    <concept>
%        <concept_id>10011007.10011006.10011008.10011024</concept_id>
%        <concept_desc>Software and its engineering~Language features</concept_desc>
%        <concept_significance>300</concept_significance>
%    </concept>
% </ccs2012>
% \end{CCSXML}

% \ccsdesc[500]{Software and its engineering~Operating systems}
% \ccsdesc[300]{Software and its engineering~Language features}

% \keywords{cgroup, eBPF, resource isolation, AI agents, operating systems}

\maketitle

\section{Introduction}

Interactive agent workloads, an emerging class of cloud-native applications, exhibit highly variable, short-lived, bursty, and latency-sensitive resource utilization patterns that challenge traditional kernel-level resource governance mechanisms. A single agent session alternates between reasoning phases, tool executions such as compilation or data processing, and interactive phases, each with distinct CPU and memory footprints. When multiple sessions share a host, a memory-intensive compilation burst in one session can trigger reclaim storms that inflate tail latency for neighbors, while fork-heavy tool invocations can exhaust CPU before static limits react. Linux cgroups, the de facto kernel abstraction for resource isolation, relies on static resource configurations and user-space reactive tuning, leaving a mismatch between the millisecond-scale dynamics of agent workloads and the coarse-grained, slow-reacting policy model of traditional cgroup configurations.

In this paper, we propose \sys, a structured kernel-level resource domain abstraction designed for agent-centric operating systems and precisely aligned with agent execution semantics including sessions and individual steps. \sys explicitly defines per-session and per-step hierarchical resource domains, provides precise resource accounting and budgeting semantics, and introduces kernel-level event signaling and soft-pause enforcement mechanisms, enabling agent workloads to adaptively respond to resource pressures.

\sys leverages eBPF, a safe and dynamically loadable in-kernel programmable execution mechanism, to embed lightweight policy functions directly into kernel enforcement points such as memory throttling paths and CPU scheduling decision hooks. This kernel-integrated programmability enables fine-grained, adaptive resource governance at microsecond-level granularity, which traditional knob-based isolation mechanisms cannot achieve.

Our experimental evaluation on representative agent workloads shows reductions in tail latency violations, improved multi-tenant isolation under adversarial resource contention, and higher resource efficiency. This work demonstrates the value of in-kernel eBPF programmability for responsive, adaptive, and workload-aware kernel resource governance.

Key contributions:
\begin{itemize}
    \item Identifies fundamental shortcomings of traditional static cgroups for interactive agents.
    \item Defines \sys as a minimal, structured, agent-aware kernel abstraction.
    \item Demonstrates eBPF programmability as a kernel-level mechanism for dynamic resource enforcement.
    \item Provides evaluation demonstrating isolation and performance improvements achieved by \sys.
\end{itemize}

\section{Background and Related Work}

\subsection{Cgroup v2 and Linux Resource Isolation}

Linux cgroup v2 provides a hierarchical resource governance abstraction where the kernel organizes tasks into a tree of control groups and applies controller-specific accounting and enforcement along that hierarchy~\cite{cgroupv2}. The memory controller exposes two key boundaries: \texttt{memory.high} as a soft throttle point that triggers reclaim pressure without invoking the OOM killer, and \texttt{memory.max} as a hard limit that triggers OOM when exceeded. Cgroup v2 also provides lifecycle controls relevant to agent execution: \texttt{cgroup.freeze} stops all processes in a subtree until explicitly unfrozen, \texttt{cgroup.kill} terminates all processes while handling concurrent forks, and \texttt{memory.oom.group} ensures OOM kills the entire cgroup atomically to avoid partial failures. These primitives provide the substrate for \sys's session-oriented resource governance.

\subsection{Programmable Kernel via eBPF}

Linux increasingly addresses the tension between standardized interfaces and dynamic workloads by introducing programmable enforcement points through eBPF, a safe and dynamically loadable mechanism for executing policy functions inside the kernel~\cite{ebpf-verifier}. This programmability extends to cgroup v2: the device controller is implemented entirely via BPF programs rather than static interface files. On the CPU side, \texttt{sched\_ext} allows scheduling policies to be defined by BPF programs with fail-safe reversion to default behavior on errors~\cite{sched-ext}. On the memory side, \texttt{memcg\_bpf\_ops} introduces hooks such as \texttt{get\_high\_delay\_ms} for custom throttle delays on \texttt{memory.high} breaches~\cite{memcg-bpf}. The kernel is converging on a pattern where cgroups define resource domains and eBPF defines domain-specific policy; \sys adopts this pattern for agent-aware resource governance.

\subsection{Recent Related Systems}

Several recent systems motivate the need for pressure-aware control and programmable policies, but none defines an agent-centric cgroup abstraction with in-kernel enforcement.

Transparent Memory Offloading (TMO) demonstrates how memory pressure signals can drive resource control at datacenter scale: the paper describes a user-space component ``Senpai'' monitoring pressure and driving offload by writing to cgroup control files~\cite{tmo}. TMO validates that cgroup interfaces are the correct kernel boundary for enforcement, but the control loop lives in user space and interacts with the kernel through coarse knobs.

On the CPU scheduler side, Mixture-of-Schedulers (ASA) proposes dynamically selecting among a portfolio of expert schedulers, switching via Linux \texttt{sched\_ext}~\cite{asa}. ASA reinforces that \texttt{sched\_ext} makes policy iteration and policy switching practical in production-like settings, but it targets general workload adaptation rather than the semantics and lifecycle constraints of agent sessions.

For agent-specific serving, Nalar proposes a framework for serving agent workflows and emphasizes that agent workloads are evolving, multi-stage, and benefit from a two-level control architecture that combines global policy computation with local event-driven enforcement~\cite{nalar}. Nalar motivates why agent workloads need structured control loops, but it operates primarily at the runtime level rather than redefining the OS-level resource domain and enforcement semantics.

\sys combines these approaches: it uses cgroup v2 as the \emph{domain} abstraction and eBPF through \texttt{sched\_ext} and \texttt{memcg\_bpf\_ops} to make enforcement low-latency and programmable inside the kernel, rather than relying solely on user-space knob tuning.

\section{Motivation: Limitations of Static Cgroup Configurations}

Agent workloads differ from conventional services along three axes that directly stress static cgroup configurations.

First, agent execution is naturally session-oriented and multi-phase. A single session alternates between reasoning/planning, tool execution, and post-processing, and these phases have very different CPU and memory footprints. Nalar's description of agent workflows with futures, dependency metadata, and evolving execution graphs is representative of this emerging class of workloads where resource needs shift over time~\cite{nalar}. A static \texttt{cpu.max}/\texttt{memory.max} budget applied uniformly to an entire session is therefore either conservative, wasting resources across phases, or aggressive, degrading latency and preventing completion under bursty phases.

Second, agent sandboxes tend to create short-lived, bursty subprocesses such as tool invocations, compilers, interpreters, and browser engines. Resource violations and interference often unfold at the same timescale as these bursts. This stresses user-space control loops that monitor and react by rewriting cgroup files: by the time the controller observes memory pressure and adjusts \texttt{memory.high} or \texttt{cpu.max}, the burst can already have caused reclaim storms or runqueue inflation that harms tail latency.

Third, static knobs have granularity and race limitations that become visible under adversarial or pathological tool behavior. The cgroup v2 documentation notes prior race issues with lowering memory limits in v1 and describes how \texttt{memory.max} mitigates this by preventing new charges then reclaiming/OOM-killing to meet the new limit. This provides robust containment, but lacks granularity for agents: hitting \texttt{memory.max} triggers cgroup OOM, which may terminate a session mid-step unless the system uses integrity semantics via \texttt{memory.oom.group}.

Cgroup v2 partially anticipates the need for external control: \texttt{memory.high} is positioned as a throttle boundary that ``never invokes the OOM killer'' and should be used when an external monitor can intervene. For agent sandboxes, however, the external monitor must be fast enough to keep up with phase changes and bursts, and it must reason in terms of agent semantics at session and step granularity, not long-lived services. The limitation of static cgroups for agent workloads lies not in cgroup v2's primitives, but in the default programming model of static configuration with user-space reactive tuning, which does not match agent dynamics.

\section{Design of \sys}

\subsection{Overview and Design Principles}

\sys defines an OS-level contract between an agent runtime and the kernel for resource governance. In this contract, the unit of governance is an agent session, optionally refined into session steps that correspond to tool invocations. Each session and step is represented as a cgroup v2 subtree, inheriting cgroup v2's hierarchical enforcement semantics where limits set higher in the tree cannot be overridden by descendants.

The design addresses the limitations identified above through six principles: (1) domain alignment, where resource domains match agent session and step boundaries; (2) fast-path enforcement, where policy executes in kernel control paths for microsecond-level reaction rather than user-space knob writes; (3) integrity semantics, providing atomic kill and cleanup to avoid partial session state; (4) graduated response, supporting throttling, soft-pause, and termination as escalating responses; (5) compatibility, building on existing cgroup v2 infrastructure without kernel modifications; and (6) safe deployment, leveraging eBPF verification~\cite{ebpf-verifier} and \texttt{sched\_ext}'s fail-safe reversion semantics~\cite{sched-ext}.

\subsection{Structured Resource Domains Aligned with Agent Sessions/Steps}

\sys maps each agent session to a cgroup v2 node, and each tool step to a child node. The parent session node represents the session's overall budget envelope; step nodes allow phase-appropriate constraints without polluting the entire session with the worst-case constraints of any step. This hierarchical structure leverages cgroup v2's guarantee that controller behaviors are hierarchical and restrictions closer to the root cannot be overridden by descendants.

\sys also uses cgroup v2 lifecycle primitives to support agent-centric recovery. For example, when a step exceeds soft limits or triggers sustained pressure, \sys can freeze the step subtree to stop execution and allow the runtime to adapt by reducing parallelism, changing tool choice, or requesting more budget. Freezing semantics, completion notification via \texttt{cgroup.events}, and the ability to later unfreeze are provided by \texttt{cgroup.freeze}. When termination is necessary due to policy violations or timeouts, \sys uses \texttt{cgroup.kill} to kill the entire subtree and avoid fork races.

\subsection{eBPF-Based Kernel Enforcement}

\sys treats the kernel as a programmable enforcement engine for agent domains, using eBPF to enable hot-path reaction without user-kernel round trips. This follows the direction established by cgroup v2's device controller, which is implemented entirely via BPF programs rather than static interface files.

On CPU scheduling, \sys relies on \texttt{sched\_ext}, which exposes a full scheduling interface to BPF programs with fail-safe reversion to default behavior on errors~\cite{sched-ext}. \sys uses this to implement an agent-session-aware scheduling policy that prioritizes latency-sensitive steps and enforces fairness across sessions.

On memory governance, \sys uses \texttt{memcg\_bpf\_ops} hooks. The key hook \texttt{get\_high\_delay\_ms} allows policy to compute a throttling delay when a cgroup breaches \texttt{memory.high}, enabling dynamic throttle functions that depend on step type and pressure signals rather than static thresholds~\cite{memcg-bpf}. The \texttt{below\_low}/\texttt{below\_min} hooks adjust protection semantics dynamically, protecting critical steps only when they exhibit working-set sensitivity.

The eBPF verifier constrains what in-kernel policies can do, reducing operational risk compared with kernel modules~\cite{ebpf-verifier}. Combined with \texttt{sched\_ext}'s fail-safe semantics, this ensures system stability when deploying programmable policies.

\section{Implementation}

\sys consists of two components: a user-space control plane comprising a library and daemon, and a kernel enforcement plane implemented in eBPF.

\subsection{Kernel-Side Implementation}

On the kernel side, \sys uses \texttt{sched\_ext} as upstream-supported infrastructure. The BPF scheduler implements \texttt{struct sched\_ext\_ops} and maintains per-session and per-step metadata in BPF maps. The key property is that \texttt{sched\_ext} is a scheduler class whose behavior is defined by BPF programs, can be enabled/disabled dynamically, and automatically reverts to the fair-class scheduler upon internal errors or stalled runnable tasks~\cite{sched-ext}. This property enables iterating on scheduling logic while maintaining system stability.

For memory, we build on the \texttt{memcg\_bpf\_ops} interface described in recent kernel work. The implementation attaches a BPF \texttt{struct\_ops} program to the memory controller and implements at least \texttt{get\_high\_delay\_ms}, with optional overrides for \texttt{below\_low}/\texttt{below\_min} for dynamic protection semantics~\cite{memcg-bpf}. We use \texttt{memory.high} as the soft boundary and \texttt{memory.max} as the hard containment boundary for buggy or malicious tools~\cite{cgroupv2}.

\subsection{User-Space Implementation}

On the user-space side, \sys provides an API to create and manage session/step cgroup subtrees, to attach per-session budgets, and to subscribe to events. The daemon consumes cgroup events and BPF-exported telemetry and updates shared BPF maps that parameterize policy functions.

\sys provides the following APIs for agent runtimes:

\begin{verbatim}
// Session lifecycle
int agentcg_session_create(config_t *cfg);
int agentcg_session_destroy(session_t *s);

// Step management
int agentcg_step_begin(session_t *s, step_t *st);
int agentcg_step_end(step_t *st);

// Budget management
int agentcg_budget_set(session_t *s, budget_t *b);
int agentcg_budget_query(session_t *s, usage_t *u);

// Event handling
int agentcg_event_subscribe(session_t *s, int fd);
int agentcg_event_ack(session_t *s, event_t *e);

// Policy loading
int agentcg_policy_load(session_t *s, bpf_prog_t *p);
\end{verbatim}

\subsection{Portability and Safety}

Portability across kernel versions is a practical concern for any eBPF-based system. \sys therefore uses libbpf and BPF CO-RE, which enables compile-once-run-everywhere portability. The kernel documentation describes CO-RE as a mechanism to address portability across kernel versions/configurations without embedding a compiler or incurring the overhead associated with approaches like BCC~\cite{libbpf}. CO-RE enables shipping \sys's BPF programs as artifacts that adapt to the target kernel's BTF at load time, assuming the required attach points and helpers are available.

Finally, \sys constrains its in-kernel logic to remain verifiable and predictable. The verifier performs control-flow checks and path exploration to ensure safe execution, which translates into an engineering constraint: policy functions must be small, bounded, and avoid complex loops or unbounded state~\cite{ebpf-verifier}. This constraint shapes the system design: high-level policy decisions remain in user space while the kernel enforces fast-path, local control laws.

\section{Evaluation}

Our evaluation is designed to answer three systems questions: (i) does \sys improve isolation and tail latency for multi-tenant agent workloads, (ii) does eBPF-based in-kernel enforcement provide benefits that static knobs and user-space control cannot match, and (iii) what is the overhead and operational risk of deploying such policies.

\subsection{Methodology}

We evaluate \sys on representative agent workloads drawn from common agent sandboxes: a code-oriented workload with compiler/test bursts, a data-analysis workload with working-set swings and memory-intensive operations, and a web/tool workload with mixed CPU and I/O phases. The workloads are executed in a multi-tenant configuration where multiple sessions share the same host and contend for CPU and memory. We instrument each session/step cgroup to collect memory events and scheduler statistics.

Baselines are chosen to isolate the role of programmability. The first baseline uses static cgroup v2 knobs: fixed \texttt{cpu.max}, fixed \texttt{memory.high}, fixed \texttt{memory.max}, relying on the default scheduler and standard memcg behavior. The second baseline adds a user-space controller similar to TMO's Senpai~\cite{tmo}, where memory pressure is monitored in user space and cgroup files are rewritten to induce reclaim or alter limits. \sys is evaluated both with its full eBPF enforcement plane and in an ablation where eBPF enforcement is disabled and only user-space knob tuning remains; comparing these configurations isolates the contribution of in-kernel programmability.

For latency and isolation, we report step-level and session-level tail latency (p95/p99), session completion rate under contention, and interference amplification measured as the performance degradation of a protected session under a noisy neighbor. For utilization, we measure CPU utilization, memory footprint, reclaim activity, and aggregate throughput under fixed hardware. Because \texttt{memory.high} is defined as a throttle boundary that never directly invokes OOM, we additionally interpret \texttt{memory.events} (high/max/oom/oom\_kill) to separate ``controlled throttling'' from ``hard failures''~\cite{cgroupv2}.

\subsection{Isolation, Latency, and Efficiency}

We measure isolation under multi-tenant contention by comparing tail latency violations across configurations, assessing whether \sys maintains isolation under adversarial noisy-neighbor scenarios. For latency, we compare policy reaction time between in-kernel enforcement and user-space monitoring, measuring the time from resource pressure detection to policy action. For efficiency, we compare aggregate throughput and memory footprint, assessing whether adaptive policies achieve better work conservation than static over-provisioning.

\subsection{Robustness and Fail-Safe Evaluation}

Robustness is evaluated under adversarial scenarios that mimic real tool failures: CPU spinning loops, fork storms, and memory blow-ups. We test containment properties such as whether the system prevents host-level collapse and whether termination or pausing remains scoped to the correct session subtree. The \texttt{cgroup.kill} guarantee of handling concurrent forks and protecting against migrations is directly relevant and can be validated experimentally by constructing fork-heavy steps and issuing kill-tree operations under load~\cite{cgroupv2}. We also evaluate integrity under memory OOM by enabling \texttt{memory.oom.group}, verifying that OOM termination respects the session domain and avoids partial kills that could leave persistent state inconsistent~\cite{cgroupv2}.

\subsection{Overhead Analysis}

We measure three categories of overhead: eBPF verification cost at policy load time, per-operation overhead on scheduling and memory control paths, and memory overhead for per-session BPF map state. We also measure verifier complexity and load time~\cite{ebpf-verifier}. On the CPU side, we validate \texttt{sched\_ext}'s fail-safe behavior by injecting scheduler errors and confirming that tasks revert to the fair-class scheduler~\cite{sched-ext}.

\section{Conclusion}

We presented \sys, a kernel-level resource abstraction designed for interactive AI agent workloads. By aligning resource domains with agent session semantics, implementing budget-based governance with soft-pause semantics, and leveraging eBPF for programmable policy enforcement, \sys improves isolation, latency, and resource efficiency.

Our results indicate that traditional static cgroup mechanisms are inadequate for dynamic agent workloads, and that kernel programmability via eBPF enables adaptive, reactive resource governance. \sys represents a step toward operating systems that understand and optimize for the characteristics of interactive AI agent workloads.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
