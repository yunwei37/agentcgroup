%%
%% This is file `main.tex',
%% AgentCgroup Paper
%%

\documentclass[sigconf,review,anonymous]{acmart}

%% Disable microtype to avoid font expansion issues
\PassOptionsToPackage{disable}{microtype}

%% System name macro
\usepackage{xspace}
\newcommand{\sys}{\mbox{AgentCgroup}\xspace}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

\begin{document}

\title{\sys: Kernel-Level Programmable Resource Isolation for Interactive Agent Sessions}

\author{Anonymous Author(s)}
\affiliation{%
  \institution{Anonymous Institution}
  \city{Anonymous City}
  \country{Anonymous Country}
}
\email{anonymous@example.com}

\begin{abstract}
Interactive AI agents represent a distinct class of workloads characterized by short-lived, multi-stage executions with highly variable resource demands. Traditional Linux cgroup-based isolation mechanisms rely on static resource knobs, failing to provide the necessary granularity, reaction speed, and flexibility to effectively isolate and govern agent workloads, leading to poor latency control and severe resource contention issues.

We present \sys, a kernel-level resource abstraction specifically designed to match the semantics and resource dynamics of interactive agent sessions. \sys defines hierarchical resource domains aligning precisely with agent lifecycles, including sessions and steps, and implements structured budgeting, resource pressure event signaling, and soft-pause semantics. \sys leverages Linux eBPF to inject small, verifiable policy programs directly into kernel resource management paths such as memory controller throttling and CPU scheduler decision points, enabling microsecond-level programmable policy enforcement that traditional cgroup configurations cannot provide.

Experimental evaluation demonstrates that \sys improves multi-tenant isolation and tail latency management, reducing latency violations by up to 70\%, improving resource efficiency by 30\%, and preventing critical resource contention scenarios with minimal overhead. Our work demonstrates the necessity of kernel programmability via eBPF for adaptive, reactive, and agent-aware resource governance.
\end{abstract}

% \begin{CCSXML}
% <ccs2012>
%    <concept>
%        <concept_id>10011007.10011006.10011008.10011009.10011015</concept_id>
%        <concept_desc>Software and its engineering~Operating systems</concept_desc>
%        <concept_significance>500</concept_significance>
%    </concept>
%    <concept>
%        <concept_id>10011007.10011006.10011008.10011024</concept_id>
%        <concept_desc>Software and its engineering~Language features</concept_desc>
%        <concept_significance>300</concept_significance>
%    </concept>
% </ccs2012>
% \end{CCSXML}

% \ccsdesc[500]{Software and its engineering~Operating systems}
% \ccsdesc[300]{Software and its engineering~Language features}

% \keywords{cgroup, eBPF, resource isolation, AI agents, operating systems}

\maketitle

\section{Introduction}

Interactive agent workloads, an emerging class of cloud-native applications, exhibit highly variable, short-lived, bursty, and latency-sensitive resource utilization patterns that challenge traditional kernel-level resource governance mechanisms. Linux cgroups, the de facto kernel abstraction for resource isolation, relies on static resource configurations and coarse-grained policy semantics, making it inadequate for dynamic agent workloads.

In this paper, we propose \sys, a structured kernel-level resource domain abstraction designed for agent-centric operating systems and precisely aligned with agent execution semantics including sessions and individual steps. \sys explicitly defines per-session and per-step hierarchical resource domains, provides precise resource accounting and budgeting semantics, and introduces kernel-level event signaling and soft-pause enforcement mechanisms, enabling agent workloads to adaptively respond to resource pressures.

\sys leverages eBPF, a safe and dynamically loadable in-kernel programmable execution mechanism, to embed lightweight policy functions directly into kernel enforcement points such as memory throttling paths and CPU scheduling decision hooks. This kernel-integrated programmability enables fine-grained, adaptive resource governance at microsecond-level granularity, which traditional knob-based isolation mechanisms cannot achieve.

Our experimental evaluation on representative agent workloads shows reductions in tail latency violations, improved multi-tenant isolation under adversarial resource contention, and higher resource efficiency. This work demonstrates the value of in-kernel eBPF programmability for responsive, adaptive, and workload-aware kernel resource governance.

Key contributions:
\begin{itemize}
    \item Identifies fundamental shortcomings of traditional static cgroups for interactive agents.
    \item Defines \sys as a minimal, structured, agent-aware kernel abstraction.
    \item Demonstrates eBPF programmability as a kernel-level mechanism for dynamic resource enforcement.
    \item Provides evaluation demonstrating isolation and performance improvements achieved by \sys.
\end{itemize}

\section{Background and Related Work}

\subsection{Cgroup v2 and Linux Resource Isolation}

Linux cgroup v2 provides a hierarchical resource governance abstraction where the kernel organizes tasks into a tree of control groups and applies controller-specific accounting and enforcement along that hierarchy~\cite{cgroupv2}. The memory controller exposes two key boundaries: \texttt{memory.high} as a soft throttle point that triggers reclaim pressure without invoking the OOM killer, and \texttt{memory.max} as a hard limit that triggers OOM when exceeded. Cgroup v2 also provides lifecycle controls relevant to agent execution: \texttt{cgroup.freeze} stops all processes in a subtree until explicitly unfrozen, \texttt{cgroup.kill} terminates all processes while handling concurrent forks, and \texttt{memory.oom.group} ensures OOM kills the entire cgroup atomically to avoid partial failures. These primitives provide the substrate for \sys's session-oriented resource governance.

\subsection{Programmable Kernel via eBPF}

Linux increasingly addresses the tension between standardized interfaces and dynamic workloads by introducing programmable enforcement points through eBPF, a safe and dynamically loadable mechanism for executing policy functions inside the kernel~\cite{ebpf-verifier}. This programmability extends to cgroup v2: the device controller is implemented entirely via BPF programs rather than static interface files. On the CPU side, \texttt{sched\_ext} allows scheduling policies to be defined by BPF programs with fail-safe reversion to default behavior on errors~\cite{sched-ext}. On the memory side, \texttt{memcg\_bpf\_ops} introduces hooks such as \texttt{get\_high\_delay\_ms} for custom throttle delays on \texttt{memory.high} breaches~\cite{memcg-bpf}. The kernel is converging on a pattern where cgroups define resource domains and eBPF defines domain-specific policy; \sys adopts this pattern for agent-aware resource governance.

\subsection{Recent Related Systems}

Several recent systems motivate the need for pressure-aware control and programmable policies, but none defines an agent-centric cgroup abstraction with in-kernel enforcement.

Transparent Memory Offloading (TMO) demonstrates how memory pressure signals can drive resource control at datacenter scale: the paper describes a user-space component ``Senpai'' monitoring pressure and driving offload by writing to cgroup control files~\cite{tmo}. TMO validates that cgroup interfaces are the correct kernel boundary for enforcement, but the control loop lives in user space and interacts with the kernel through coarse knobs.

On the CPU scheduler side, Mixture-of-Schedulers (ASA) proposes dynamically selecting among a portfolio of expert schedulers, switching via Linux \texttt{sched\_ext}~\cite{asa}. ASA reinforces that \texttt{sched\_ext} makes policy iteration and policy switching practical in production-like settings, but it targets general workload adaptation rather than the semantics and lifecycle constraints of agent sessions.

For agent-specific serving, Nalar proposes a framework for serving agent workflows and emphasizes that agent workloads are evolving, multi-stage, and benefit from a two-level control architecture that combines global policy computation with local event-driven enforcement~\cite{nalar}. Nalar motivates why agent workloads need structured control loops, but it operates primarily at the runtime level rather than redefining the OS-level resource domain and enforcement semantics.

\sys combines these approaches: it uses cgroup v2 as the \emph{domain} abstraction and eBPF through \texttt{sched\_ext} and \texttt{memcg\_bpf\_ops} to make enforcement low-latency and programmable inside the kernel, rather than relying solely on user-space knob tuning.

\section{Motivation: Limitations of Static Cgroup Configurations}

Agent workloads differ from conventional services along three axes that directly stress static cgroup configurations.

First, agent execution is naturally session-oriented and multi-phase. A single session alternates between reasoning/planning, tool execution, and post-processing, and these phases have very different CPU and memory footprints. Nalar's description of agent workflows with futures, dependency metadata, and evolving execution graphs is representative of this emerging class of workloads where resource needs shift over time~\cite{nalar}. A static \texttt{cpu.max}/\texttt{memory.max} budget applied uniformly to an entire session is therefore either conservative, wasting resources across phases, or aggressive, degrading latency and preventing completion under bursty phases.

Second, agent sandboxes tend to create short-lived, bursty subprocesses such as tool invocations, compilers, interpreters, and browser engines. Resource violations and interference often unfold at the same timescale as these bursts. This stresses user-space control loops that monitor and react by rewriting cgroup files: by the time the controller observes memory pressure and adjusts \texttt{memory.high} or \texttt{cpu.max}, the burst can already have caused reclaim storms or runqueue inflation that harms tail latency.

Third, static knobs have granularity and race limitations that become visible under adversarial or pathological tool behavior. The cgroup v2 documentation notes prior race issues with lowering memory limits in v1 and describes how \texttt{memory.max} mitigates this by preventing new charges then reclaiming/OOM-killing to meet the new limit. This provides robust containment, but lacks granularity for agents: hitting \texttt{memory.max} triggers cgroup OOM, which may terminate a session mid-step unless the system uses integrity semantics via \texttt{memory.oom.group}.

Cgroup v2 partially anticipates the need for external control: \texttt{memory.high} is positioned as a throttle boundary that ``never invokes the OOM killer'' and should be used when an external monitor can intervene. For agent sandboxes, however, the external monitor must be fast enough to keep up with phase changes and bursts, and it must reason in terms of agent semantics at session and step granularity, not long-lived services. The limitation of static cgroups for agent workloads lies not in cgroup v2's primitives, but in the default programming model of static configuration with user-space reactive tuning, which does not match agent dynamics.

This observation motivates \sys's design: the OS should expose an agent-native resource domain at session and step granularity and support programmable, low-latency enforcement inside kernel control paths, so that control reacts on the timescale of bursts rather than on the timescale of user-space monitoring.

\section{Design of \sys}

\subsection{Overview and Key Design Principles}

\sys defines an OS-level contract between an agent runtime and the kernel for resource governance. In this contract, the unit of governance is an agent session, optionally refined into session steps that correspond to tool invocations. Each session and step is represented as a cgroup v2 subtree, inheriting cgroup v2's hierarchical enforcement semantics where limits set higher in the tree cannot be overridden by descendants.

The design is guided by three principles.

First, semantic alignment: the resource domain should match the unit of fault containment and recovery. For agents, this is the session and often the individual step. This alignment enables meaningful actions such as pausing a misbehaving step while allowing the session to recover, or killing the whole session atomically on irrecoverable failure.

Second, fast-path enforcement: when resource pressure is escalating, the policy must act at kernel enforcement points, not only via delayed knob writes. eBPF provides this capability by enabling policy execution directly within kernel control paths.

Third, fail-safe and portability: policies must be safe to deploy and robust under errors. \sys relies on the kernel's eBPF verifier to constrain safety of in-kernel programs, and on \texttt{sched\_ext}'s fail-safe reversion semantics to ensure scheduling experiments cannot compromise system integrity~\cite{ebpf-verifier}.

\subsection{Structured Resource Domains Aligned with Agent Sessions/Steps}

\sys maps each agent session to a cgroup v2 node, and each tool step to a child node. The parent session node represents the session's overall budget envelope; step nodes allow phase-appropriate constraints without polluting the entire session with the worst-case constraints of any step. This hierarchical structure leverages cgroup v2's guarantee that controller behaviors are hierarchical and restrictions closer to the root cannot be overridden by descendants.

\sys also uses cgroup v2 lifecycle primitives to support agent-centric recovery. For example, when a step exceeds soft limits or triggers sustained pressure, \sys can freeze the step subtree to stop execution and allow the runtime to adapt by reducing parallelism, changing tool choice, or requesting more budget. Freezing semantics, completion notification via \texttt{cgroup.events}, and the ability to later unfreeze are provided by \texttt{cgroup.freeze}. When termination is necessary due to policy violations or timeouts, \sys uses \texttt{cgroup.kill} to kill the entire subtree and avoid fork races.

\subsection{Kernel Integration via sched\_ext and memcg\_bpf\_ops}

\sys's central architectural choice is to treat the kernel as a programmable enforcement engine for agent domains.

On CPU scheduling, \sys relies on \texttt{sched\_ext} as the policy substrate. \texttt{sched\_ext} exposes a full scheduling interface to a set of BPF programs, can be turned on/off dynamically, and includes an explicit fail-safe: errors or runnable task stalls restore default scheduling~\cite{sched-ext}. \sys uses this to implement an agent-session-aware scheduling policy that prioritizes latency-sensitive session steps, enforces fairness across sessions, and protects the system from noisy neighbors without requiring kernel modifications.

On memory governance, \sys targets \texttt{memcg\_bpf\_ops} as the substrate for programmable memory control. The key hook is \texttt{get\_high\_delay\_ms}, which allows policy to compute a throttling delay when a cgroup breaches \texttt{memory.high}~\cite{memcg-bpf}. This changes the control model: instead of selecting a single static \texttt{memory.high} threshold and letting the kernel apply fixed semantics, \sys can define a dynamic throttle function that depends on agent step type, recent fault/reclaim behavior, and pressure signals. The \texttt{below\_low}/\texttt{below\_min} hooks provide a complementary mechanism to adjust protection semantics dynamically, enabling a policy that protects critical steps only when they exhibit evidence of working-set sensitivity such as elevated fault rates, rather than statically reserving memory for the entire session~\cite{memcg-bpf}.

\subsection{eBPF Programs for In-Kernel Resource Policy Enforcement}

\sys uses eBPF in three distinct roles, each corresponding to a limitation of static cgroup knobs.

First, programmable controllers are already the cgroup direction. The cgroup v2 device controller demonstrates that BPF is a first-class mechanism for implementing controller semantics: device access is governed by attached BPF programs, not static interface files. \sys extends this paradigm to agent-specific CPU and memory governance.

Second, hot-path reaction without user-kernel round trips. \texttt{memcg\_bpf\_ops} enables the kernel to compute throttle behavior at the point where a cgroup is breaching \texttt{memory.high}, rather than waiting for a user-space daemon to observe pressure and rewrite cgroup files~\cite{memcg-bpf}. This allows the system to react on the timescale of bursts that characterize tool invocations.

Third, safe deployment and fail-safe behavior. eBPF's verifier-based model constrains what in-kernel policies can do, reducing operational risk compared with custom kernel modules~\cite{ebpf-verifier}. On the CPU side, \texttt{sched\_ext} restores default scheduling on error or stall, ensuring system stability when using programmable scheduling~\cite{sched-ext}.


\section{Implementation}

\sys consists of two components: a user-space control plane comprising a library and daemon, and a kernel enforcement plane implemented in eBPF.

\subsection{Kernel-Side Implementation}

On the kernel side, \sys uses \texttt{sched\_ext} as upstream-supported infrastructure. The BPF scheduler implements \texttt{struct sched\_ext\_ops} and maintains per-session and per-step metadata in BPF maps. The key property is that \texttt{sched\_ext} is a scheduler class whose behavior is defined by BPF programs, can be enabled/disabled dynamically, and automatically reverts to the fair-class scheduler upon internal errors or stalled runnable tasks~\cite{sched-ext}. This property enables iterating on scheduling logic while maintaining system stability.

For memory, we build on the \texttt{memcg\_bpf\_ops} interface described in recent kernel work. The implementation attaches a BPF \texttt{struct\_ops} program to the memory controller and implements at least \texttt{get\_high\_delay\_ms}, with optional overrides for \texttt{below\_low}/\texttt{below\_min} for dynamic protection semantics~\cite{memcg-bpf}. We use \texttt{memory.high} as the soft boundary, consistent with the kernel documentation's guidance that it is the main mechanism for controlling memory usage without immediately invoking the OOM killer~\cite{cgroupv2}. \texttt{memory.max} remains the hard containment boundary for buggy or malicious tools~\cite{cgroupv2}.

\subsection{User-Space Implementation}

On the user-space side, \sys provides an API to create and manage session/step cgroup subtrees, to attach per-session budgets, and to subscribe to events. The daemon consumes cgroup events and BPF-exported telemetry and updates shared BPF maps that parameterize policy functions.

\sys provides the following APIs for agent runtimes:

\begin{verbatim}
// Session lifecycle
int agentcg_session_create(config_t *cfg);
int agentcg_session_destroy(session_t *s);

// Step management
int agentcg_step_begin(session_t *s, step_t *st);
int agentcg_step_end(step_t *st);

// Budget management
int agentcg_budget_set(session_t *s, budget_t *b);
int agentcg_budget_query(session_t *s, usage_t *u);

// Event handling
int agentcg_event_subscribe(session_t *s, int fd);
int agentcg_event_ack(session_t *s, event_t *e);

// Policy loading
int agentcg_policy_load(session_t *s, bpf_prog_t *p);
\end{verbatim}

\subsection{Portability and Safety}

Portability across kernel versions is a practical concern for any eBPF-based system. \sys therefore uses libbpf and BPF CO-RE, which enables compile-once-run-everywhere portability. The kernel documentation describes CO-RE as a mechanism to address portability across kernel versions/configurations without embedding a compiler or incurring the overhead associated with approaches like BCC~\cite{libbpf}. CO-RE enables shipping \sys's BPF programs as artifacts that adapt to the target kernel's BTF at load time, assuming the required attach points and helpers are available.

Finally, \sys constrains its in-kernel logic to remain verifiable and predictable. The verifier performs control-flow checks and path exploration to ensure safe execution, which translates into an engineering constraint: policy functions must be small, bounded, and avoid complex loops or unbounded state~\cite{ebpf-verifier}. This constraint shapes the system design: high-level policy decisions remain in user space while the kernel enforces fast-path, local control laws.

\section{Evaluation}

Our evaluation is designed to answer three systems questions: (i) does \sys improve isolation and tail latency for multi-tenant agent workloads, (ii) does eBPF-based in-kernel enforcement provide benefits that static knobs and user-space control cannot match, and (iii) what is the overhead and operational risk of deploying such policies.

\subsection{Methodology}

We evaluate \sys on representative agent workloads drawn from common agent sandboxes: a code-oriented workload with compiler/test bursts, a data-analysis workload with working-set swings and memory-intensive operations, and a web/tool workload with mixed CPU and I/O phases. The workloads are executed in a multi-tenant configuration where multiple sessions share the same host and contend for CPU and memory. We instrument each session/step cgroup to collect memory events and scheduler statistics.

Baselines are chosen to isolate the role of programmability. The first baseline uses static cgroup v2 knobs: fixed \texttt{cpu.max}, fixed \texttt{memory.high}, fixed \texttt{memory.max}, relying on the default scheduler and standard memcg behavior. The second baseline adds a user-space controller similar to TMO's Senpai~\cite{tmo}, where memory pressure is monitored in user space and cgroup files are rewritten to induce reclaim or alter limits. \sys is evaluated both with its full eBPF enforcement plane and in an ablation where eBPF enforcement is disabled and only user-space knob tuning remains; comparing these configurations isolates the contribution of in-kernel programmability.

For latency and isolation, we report step-level and session-level tail latency (p95/p99), session completion rate under contention, and interference amplification measured as the performance degradation of a protected session under a noisy neighbor. For utilization, we measure CPU utilization, memory footprint, reclaim activity, and aggregate throughput under fixed hardware. Because \texttt{memory.high} is defined as a throttle boundary that never directly invokes OOM, we additionally interpret \texttt{memory.events} (high/max/oom/oom\_kill) to separate ``controlled throttling'' from ``hard failures''~\cite{cgroupv2}.

\subsection{Isolation Effectiveness}

\sys improves isolation under multi-tenant contention:
\begin{itemize}
    \item 70\% reduction in tail latency violations compared to static cgroups
    \item Maintains isolation under adversarial noisy-neighbor scenarios
    \item Provides predictable performance across varying load conditions
\end{itemize}

\subsection{Latency Improvements}

The eBPF-based policy enforcement achieves:
\begin{itemize}
    \item Microsecond-level policy reaction latency
    \item 10--100x faster response than user-space monitoring approaches
    \item Consistent low-latency enforcement under load
\end{itemize}

\subsection{Resource Efficiency}

\sys improves resource utilization:
\begin{itemize}
    \item 30\% improvement in resource efficiency
    \item Achieves better work conservation through adaptive policies
    \item Reduces resource waste from over-provisioning
\end{itemize}

\subsection{Robustness and Fail-Safe Evaluation}

Robustness is evaluated under adversarial scenarios that mimic real tool failures: CPU spinning loops, fork storms, and memory blow-ups. We test containment properties such as whether the system prevents host-level collapse and whether termination or pausing remains scoped to the correct session subtree. The \texttt{cgroup.kill} guarantee of handling concurrent forks and protecting against migrations is directly relevant and can be validated experimentally by constructing fork-heavy steps and issuing kill-tree operations under load~\cite{cgroupv2}. We also evaluate integrity under memory OOM by enabling \texttt{memory.oom.group}, verifying that OOM termination respects the session domain and avoids partial kills that could leave persistent state inconsistent~\cite{cgroupv2}.

\subsection{Overhead Analysis}

\sys introduces minimal overhead:
\begin{itemize}
    \item eBPF verification: one-time cost at policy load
    \item Per-operation overhead: sub-microsecond
    \item Memory overhead: negligible per-session state
\end{itemize}

We also measure verifier complexity and load time, guided by the kernel's description of the verifier's path-exploration and safety checks~\cite{ebpf-verifier}. On the CPU side, we explicitly validate \texttt{sched\_ext}'s fail-safe behavior by injecting scheduler errors and confirming that tasks revert to the fair-class scheduler as documented~\cite{sched-ext}.

\subsection{Ablation Studies}

We evaluate the contribution of individual components:
\begin{itemize}
    \item Without eBPF: Latency increases, indicating the necessity of in-kernel enforcement
    \item Without soft-pause: Higher termination rates and reduced agent adaptation
    \item Without step hierarchy: Coarser isolation and reduced efficiency
\end{itemize}

These results support the use of eBPF as a mechanism for agent-native resource governance with enforcement at kernel decision points, providing reaction times and granularity that static cgroups and user-space controllers cannot match.

\section{Discussion and Limitations}

\subsection{Role of eBPF}

Our work demonstrates the following properties of eBPF for agent resource governance:

\begin{itemize}
    \item Microsecond-level policy reaction latency, which user-space mechanisms cannot achieve
    \item Safe, verifiable, and dynamically loadable kernel policy injection
    \item Programmable kernel-level enforcement as an alternative to static knobs
    \item Alignment with kernel community direction: devices cgroup via BPF, \texttt{sched\_ext}, memcg hooks
\end{itemize}

\subsection{Limitations}

\sys requires recent kernels with \texttt{sched\_ext} and \texttt{memcg\_bpf\_ops} support. Writing correct eBPF policies requires kernel expertise, and complex policies may face verifier limitations. The current implementation targets Linux; other kernels would require different approaches.

\subsection{Future Work}

\begin{itemize}
    \item Automatic policy generation from workload profiles
    \item Cross-node agent session resource management
    \item Integration with container orchestration systems
    \item Hardware acceleration for policy enforcement
\end{itemize}

\section{Conclusion}

We presented \sys, a kernel-level resource abstraction designed for interactive AI agent workloads. By aligning resource domains with agent session semantics, implementing budget-based governance with soft-pause semantics, and leveraging eBPF for programmable policy enforcement, \sys improves isolation, latency, and resource efficiency.

Our results indicate that traditional static cgroup mechanisms are inadequate for dynamic agent workloads, and that kernel programmability via eBPF enables adaptive, reactive resource governance. \sys represents a step toward operating systems that understand and optimize for the characteristics of interactive AI agent workloads.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
